{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Fit a ridge model with motion energy features\n\n\nIn this example, we model the fMRI responses with motion-energy features\nextracted from the movie stimulus. The model is a regularized linear regression\nmodel.\n\nThis tutorial reproduces part of the analysis described in Nishimoto et al\n(2011) [1]_. See this publication for more details about the experiment, the\nmotion-energy features, along with more results and more discussions.\n\n*Motion-energy features:* Motion-energy features result from filtering a video\nstimulus with spatio-temporal Gabor filters. A pyramid of filters is used to\ncompute the motion-energy features at multiple spatial and temporal scales.\nMotion-energy features were introduced in [1]_.\n\n*Summary:* We first concatenate the features with multiple delays, to account\nfor the slow hemodynamic response. A linear regression model then weights each\ndelayed feature with a different weight, to build a predictive model of BOLD\nactivity. Again, the linear regression is regularized to improve robustness to\ncorrelated features and to improve generalization. The optimal regularization\nhyperparameter is selected independently on each voxel over a grid-search with\ncross-validation. Finally, the model generalization performance is evaluated on\na held-out test set, comparing the model predictions with the ground-truth fMRI\nresponses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the data\n-------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# path of the data directory\nimport os\nfrom voxelwise_tutorials.io import get_data_home\ndirectory = os.path.join(get_data_home(), \"vim-2\")\nprint(directory)\n\n# modify to use another subject\nsubject = \"subject1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here the data is not loaded in memory, we only take a peak at the data shape.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import h5py\nimport os.path as op\n\nwith h5py.File(op.join(directory, f'VoxelResponses_{subject}.mat'), 'r') as f:\n    print(f.keys())  # Show all variables\n    for key in f.keys():\n        print(f[key])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we load the fMRI responses.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom voxelwise_tutorials.io import load_hdf5_array\n\nfile_name = op.join(directory, f'VoxelResponses_{subject}.mat')\nY_train = load_hdf5_array(file_name, key='rt')\nY_test_repeats = load_hdf5_array(file_name, key='rva')\n\n# transpose to fit in scikit-learn's API\nY_train = Y_train.T\nY_test_repeats = np.transpose(Y_test_repeats, [1, 2, 0])\n\n# Change to True to select only voxels from (e.g.) left V1 (\"v1lh\");\n# Otherwise, all voxels will be modeled.\nif False:\n    roi = load_hdf5_array(file_name, key='/roi/v1lh').ravel()\n    mask = (roi == 1)\n    Y_train = Y_train[:, mask]\n    Y_test_repeats = Y_test_repeats[:, :, mask]\n\n# Z-score test runs, since the mean and scale of fMRI responses changes for\n# each run. The train runs are already zscored.\nY_test_repeats -= np.mean(Y_test_repeats, axis=1, keepdims=True)\nY_test_repeats /= np.std(Y_test_repeats, axis=1, keepdims=True)\n\n# Average test repeats, since we cannot model the non-repeatable part of\n# fMRI responses.\nY_test = Y_test_repeats.mean(0)\n\n# remove nans, mainly present on non-cortical voxels\nY_train = np.nan_to_num(Y_train)\nY_test = np.nan_to_num(Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we load the motion-energy features, that are going to be used for the\nlinear regression model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "file_name = op.join(directory, \"features\", \"motion_energy.hdf\")\nX_train = load_hdf5_array(file_name, key='X_train')\nX_test = load_hdf5_array(file_name, key='X_test')\n\n# We use single precision float to speed up model fitting on GPU.\nX_train = X_train.astype(\"float32\")\nX_test = X_test.astype(\"float32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the cross-validation scheme\n----------------------------------\n\nTo select the best hyperparameter through cross-validation, we must define a\ntrain-validation splitting scheme. Since fMRI time-series are autocorrelated\nin time, we should preserve as much as possible the time blocks.\nIn other words, since consecutive time samples are correlated, we should not\nput one time sample in the training set and the immediately following time\nsample in the validation set. Thus, we define here a leave-one-run-out\ncross-validation split, which preserves each recording run.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import check_cv\nfrom voxelwise_tutorials.utils import generate_leave_one_run_out\n\nn_samples_train = X_train.shape[0]\n\n# indice of first sample of each run, each run having 600 samples\nrun_onsets = np.arange(0, n_samples_train, 600)\n\n# define a cross-validation splitter, compatible with scikit-learn\ncv = generate_leave_one_run_out(n_samples_train, run_onsets)\ncv = check_cv(cv)  # copy the splitter into a reusable list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the model\n----------------\n\nNow, let's define the model pipeline.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# Display the scikit-learn pipeline with an HTML diagram.\nfrom sklearn import set_config\nset_config(display='diagram')  # requires scikit-learn 0.23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With one target, we could directly use the pipeline in scikit-learn's\nGridSearchCV, to select the optimal hyperparameters over cross-validation.\nHowever, GridSearchCV can only optimize one score. Thus, in the multiple\ntarget case, GridSearchCV can only optimize e.g. the mean score over targets.\nHere, we want to find a different optimal hyperparameter per target/voxel, so\nwe use himalaya's KernelRidgeCV instead.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from himalaya.kernel_ridge import KernelRidgeCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first concatenate the features with multiple delays, to account for the\nhemodynamic response. The linear regression model will then weight each\ndelayed feature with a different weight, to build a predictive model.\n\nWith a sample every 1 second, we use 8 delays [1, 2, 3, 4, 5, 6, 7, 8] to\ncover the most part of the hemodynamic response peak.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.delayer import Delayer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The package``himalaya`` implements different computational backends,\nincluding GPU backends. The available GPU backends are \"torch_cuda\" and\n\"cupy\". (These backends are only available if you installed the corresponding\npackage with CUDA enabled. Check the pytorch/cupy documentation for install\ninstructions.)\n\nHere we use the \"torch_cuda\" backend, but if the import fails we continue\nwith the default \"numpy\" backend. The \"numpy\" backend is expected to be\nslower since it only uses the CPU.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from himalaya.backend import set_backend\nbackend = set_backend(\"torch_cuda\", on_error=\"warn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The scale of the regularization hyperparameter alpha is unknown, so we use\na large logarithmic range, and we will check after the fit that best\nhyperparameters are not all on one range edge.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "alphas = np.logspace(1, 20, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The scikit-learn Pipeline can be used as a regular estimator, calling\npipeline.fit, pipeline.predict, etc.\nUsing a pipeline can be useful to clarify the different steps, avoid\ncross-validation mistakes, or automatically cache intermediate results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline = make_pipeline(\n    StandardScaler(with_mean=True, with_std=False),\n    Delayer(delays=[1, 2, 3, 4, 5, 6, 7, 8]),\n    KernelRidgeCV(\n        alphas=alphas, cv=cv,\n        solver_params=dict(n_targets_batch=500, n_alphas_batch=5,\n                           n_targets_batch_refit=100)),\n)\npipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the model\n-------------\n\nWe fit on the train set, and score on the test set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline.fit(X_train, Y_train)\n\nscores = pipeline.score(X_test, Y_test)\n# Since we performed the KernelRidgeCV on GPU, scores are returned as\n# torch.Tensor on GPU. Thus, we need to move them into numpy arrays on CPU, to\n# be able to use them e.g. in a matplotlib figure.\nscores = backend.to_numpy(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the scale of alphas is unknown, we plot the optimal alphas selected by\nthe solver over cross-validation. This plot is helpful to refine the alpha\ngrid if the range is too small or too large.\n\nNote that some voxels are at the maximum regularization of the grid. These\nare voxels where the model has no predictive power, and where the optimal\nregularization is large to lead to a prediction equal to zero.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom himalaya.viz import plot_alphas_diagnostic\n\nplot_alphas_diagnostic(best_alphas=backend.to_numpy(pipeline[-1].best_alphas_),\n                       alphas=alphas)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare with a model without delays\n-----------------------------------\n\n# To present an example of model comparison, we define here another model,\nwithout feature delays (i.e. no Delayer). This model is unlikely to perform\nwell, since fMRI responses are delayed in time with respect to the stimulus.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline_nodelay = make_pipeline(\n    StandardScaler(with_mean=True, with_std=False),\n    KernelRidgeCV(\n        alphas=alphas, cv=cv,\n        solver_params=dict(n_targets_batch=500, n_alphas_batch=5,\n                           n_targets_batch_refit=100)),\n)\npipeline\n\npipeline_nodelay.fit(X_train, Y_train)\nscores_nodelay = pipeline_nodelay.score(X_test, Y_test)\nscores_nodelay = backend.to_numpy(scores_nodelay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we plot the comparison of model performances with a 2D histogram. All\n~70k voxels are represented in this histogram, where the diagonal corresponds\nto identical performance for both models. A distibution deviating from the\ndiagonal means that one model has better predictive performances than the\nother.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.viz import plot_hist2d\n\nax = plot_hist2d(scores_nodelay, scores)\nax.set(title='Generalization R2 scores', xlabel='model without delays',\n       ylabel='model with delays')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n----------\n\n.. [1] Nishimoto, S., Vu, A. T., Naselaris, T., Benjamini, Y., Yu,\n    B., & Gallant, J. L. (2011). Reconstructing visual experiences from brain\n    activity evoked by natural movies. Current Biology, 21(19), 1641-1646.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}