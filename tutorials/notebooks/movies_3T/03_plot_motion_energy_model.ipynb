{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Fit a ridge model with motion energy features\n\n\nIn this second example, we model the fMRI responses with motion-energy features\nextracted from the movie stimulus. The model is still a regularized linear\nregression model.\n\nThis tutorial reproduces part of the analysis described in Nishimoto et al\n(2011) [1]_. See this publication for more details about the experiment, the\nmotion-energy features, along with more results and more discussions.\n\nMotion-energy features result from filtering a video stimulus with\nspatio-temporal Gabor filters. A pyramid of filters is used to compute the\nmotion-energy features at multiple spatial and temporal scales.\n\nAs in the previous example, we first concatenate the features with multiple\ndelays, to account for the hemodynamic response. A linear regression model\nthen weights each delayed feature with a different weight, to build a\npredictive model of BOLD activity.\nAgain, the linear regression is regularized to improve robustness to correlated\nfeatures and to improve generalization. The optimal regularization\nhyperparameter is selected over a grid-search with cross-validation.\nFinally, the model generalization performance is evaluated on a held-out test\nset, comparing the model predictions with the corresponding ground-truth fMRI\nresponses.\n\nThe ridge model is fitted with the package\n`himalaya <https://github.com/gallantlab/himalaya>`_.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# path of the data directory\nimport os\nfrom voxelwise_tutorials.io import get_data_home\ndirectory = os.path.join(get_data_home(), \"vim-4\")\nprint(directory)\n\n# modify to use another subject\nsubject = \"S01\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the data\n-------------\n\nWe first load the fMRI responses.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom voxelwise_tutorials.io import load_hdf5_array\n\nfile_name = os.path.join(directory, \"responses\", f\"{subject}_responses.hdf\")\nY_train = load_hdf5_array(file_name, key=\"Y_train\")\nY_test = load_hdf5_array(file_name, key=\"Y_test\")\nrun_onsets = load_hdf5_array(file_name, key=\"run_onsets\")\n\n# We average the test repeats, since we cannot model the non-repeatable part of\n# fMRI responses. It means that the prediction :math:`R^2` scores will be\n# relative to the explainable variance.\nY_test = Y_test.mean(0)\n\n# We remove NaN values present on non-cortical voxels.\nY_train = np.nan_to_num(Y_train)\nY_test = np.nan_to_num(Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we load the \"motion-energy\" features, that we will\nuse for the linear regression model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feature_space = \"motion_energy\"\nfile_name = os.path.join(directory, \"features\", f\"{feature_space}.hdf\")\nX_train = load_hdf5_array(file_name, key=\"X_train\")\nX_test = load_hdf5_array(file_name, key=\"X_test\")\n\n# We use single precision float to speed up model fitting on GPU.\nX_train = X_train.astype(\"float32\")\nX_test = X_test.astype(\"float32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the cross-validation scheme\n----------------------------------\n\nWe define the same leave-one-run-out cross-validation split as in the\nprevious example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import check_cv\nfrom voxelwise_tutorials.utils import generate_leave_one_run_out\n\n# indice of first sample of each run\nrun_onsets = load_hdf5_array(file_name, key=\"run_onsets\")\n\n# define a cross-validation splitter, compatible with ``scikit-learn``` API\nn_samples_train = X_train.shape[0]\ncv = generate_leave_one_run_out(n_samples_train, run_onsets)\ncv = check_cv(cv)  # copy the cross-validation splitter into a reusable list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the model\n----------------\n\nWe define the same model as in the previous example. See the previous\nexample for more details about the model definition.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom voxelwise_tutorials.delayer import Delayer\nfrom himalaya.kernel_ridge import KernelRidgeCV\nfrom himalaya.backend import set_backend\nbackend = set_backend(\"torch_cuda\", on_error=\"warn\")\n\nalphas = np.logspace(1, 20, 20)\n\npipeline_motion_energy = make_pipeline(\n    StandardScaler(with_mean=True, with_std=False),\n    Delayer(delays=[1, 2, 3, 4]),\n    KernelRidgeCV(\n        alphas=alphas, cv=cv,\n        solver_params=dict(n_targets_batch=500, n_alphas_batch=5,\n                           n_targets_batch_refit=100)),\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn import set_config\nset_config(display='diagram')\npipeline_motion_energy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the model\n-------------\n\nWe fit on the train set, and score on the test set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline_motion_energy.fit(X_train, Y_train)\n\nscores_motion_energy = pipeline_motion_energy.score(X_test, Y_test)\nscores_motion_energy = backend.to_numpy(scores_motion_energy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the model performances\n---------------------------\nThe performances are computed using the math:`R^2` scores.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom voxelwise_tutorials.viz import plot_flatmap_from_mapper\n\nmapper_file = os.path.join(directory, \"mappers\", f\"{subject}_mappers.hdf\")\nax = plot_flatmap_from_mapper(scores_motion_energy, mapper_file, vmin=0,\n                              vmax=0.5)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The motion-energy features lead to large generalization scores in the\nearly visual cortex (V1, V2? V3, ...). For more discussions about these\nresults, we refer the reader to the original publication [1]_.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare with the wordnet model\n------------------------------\n\nIt is interesting to compare the performances of this motion-energy model\nto the performances of the wordnet model fitted in the previous example.\nTo compare them, we first need to fit again the semantic wordnet model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feature_space = \"wordnet\"\nfile_name = os.path.join(directory, \"features\", f\"{feature_space}.hdf\")\nX_train = load_hdf5_array(file_name, key=\"X_train\")\nX_test = load_hdf5_array(file_name, key=\"X_test\")\n\n# We use single precision float to speed up model fitting on GPU.\nX_train = X_train.astype(\"float32\")\nX_test = X_test.astype(\"float32\")\n\n# We can create an unfitted copy of the pipeline with the `clone` function.\nfrom sklearn.base import clone\npipeline_wordnet = clone(pipeline_motion_energy)\npipeline_wordnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline_wordnet.fit(X_train, Y_train)\nscores_wordnet = pipeline_wordnet.score(X_test, Y_test)\nscores_wordnet = backend.to_numpy(scores_wordnet)\n\nax = plot_flatmap_from_mapper(scores_wordnet, mapper_file, vmin=0,\n                              vmax=0.5)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also plot the comparison of model performances with a 2D histogram.\nAll ~70k voxels are represented in this histogram, where the diagonal\ncorresponds to identical performance for both models. A distibution deviating\nfrom the diagonal means that one model has better predictive performances\nthan the other.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.viz import plot_hist2d\n\nax = plot_hist2d(scores_wordnet, scores_motion_energy)\nax.set(title='Generalization R2 scores', xlabel='semantic wordnet model',\n       ylabel='motion energy model')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interestingly, the well predicted voxels are different in the two models.\nTo further describe these differences, we can plot both performances on the\nsame flatmap.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.viz import plot_2d_flatmap_from_mapper\n\nmapper_file = os.path.join(directory, \"mappers\", f\"{subject}_mappers.hdf\")\nax = plot_2d_flatmap_from_mapper(scores_wordnet, scores_motion_energy,\n                                 mapper_file, vmin=0, vmax=0.5, vmin2=0,\n                                 vmax2=0.5, label_1=\"wordnet\",\n                                 label_2=\"motion energy\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The blue regions are well predicted by the motion-energy features,\nthe orange regions are well predicted by the wordnet features,\nand the white regions are well predicted by both feature spaces.\n\nInterestingly, a large part of the visual semantic areas are not only well\npredicted by the wordnet features, but also by the motion-energy features,\nas indicated by the white color. Since these two features spaces encode\nquite different information, two interpretations are possible.\nIn the first one, the two feature spaces encode complementary information,\nand could be used jointly to further increase the generalization\nperformances. In the second interpretation, both feature spaces encode the\nsame information, because of spurious correlation in the stimulus. For\nexample, all faces in the stimulus might be located in the same part of the\nvisual field, thus a motion-energy feature at this location might contain\nall the necessary information to predict the presence of a face, without\nspecifically encoding for the semantic of faces.\n\nTo better disentangle the two feature spaces, we developed a joint model\ncalled `banded ridge regression` [2]_, which fits multiple feature spaces\nsimultaneously with optimal regularization for each feature space.\nThis model is described in the next example.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n----------\n\n.. [1] Nishimoto, S., Vu, A. T., Naselaris, T., Benjamini, Y., Yu,\n    B., & Gallant, J. L. (2011). Reconstructing visual experiences from brain\n    activity evoked by natural movies. Current Biology, 21(19), 1641-1646.\n\n.. [2] Nunez-Elizalde, A. O., Huth, A. G., & Gallant, J. L. (2019).\n    Voxelwise encoding models with non-spherical multivariate normal priors.\n    Neuroimage, 197, 482-492.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}