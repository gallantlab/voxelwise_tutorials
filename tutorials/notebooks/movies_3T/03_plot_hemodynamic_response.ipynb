{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Visualize the hemodynamic response\n\n\nIn this example, we describe how the hemodynamic response is estimated in the\nprevious model. We fit the same ridge model as in the previous example, and\nfurther describe the need to delay the features in time.\n\nAs explained in previous example, the BOLD signal recorded in fMRI experiments\nis delayed in time with respect to the stimulus. With different delayed\nversions of the features, the linear regression model weight each delayed\nfeature with a different weight, to maximize the predictions. With a sample\nevery 2 seconds, we typically use 4 delays [1, 2, 3, 4] to cover the most part\nof the hemodynamic response peak.\n\nIn this example, we show the descrease in prediction performances when using no\ndelays. We also show how to visualize the estimated hemodynamic response\nfunction (HRF) using more delays.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Path of the data directory\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nfrom voxelwise_tutorials.io import get_data_home\ndirectory = os.path.join(get_data_home(), \"vim-4\")\nprint(directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# modify to use another subject\nsubject = \"S01\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the data\n-------------\n\nWe first load the fMRI responses.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom voxelwise_tutorials.io import load_hdf5_array\n\nfile_name = os.path.join(directory, \"responses\", f\"{subject}_responses.hdf\")\nY_train = load_hdf5_array(file_name, key=\"Y_train\")\nY_test = load_hdf5_array(file_name, key=\"Y_test\")\n\nprint(\"(n_samples_train, n_voxels) =\", Y_train.shape)\nprint(\"(n_repeats, n_samples_test, n_voxels) =\", Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We average the test repeats, to remove the non-repeatable part of fMRI\nresponses.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Y_test = Y_test.mean(0)\n\nprint(\"(n_samples_test, n_voxels) =\", Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We fill potential NaN (not-a-number) values with zeros.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Y_train = np.nan_to_num(Y_train)\nY_test = np.nan_to_num(Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we load the semantic \"wordnet\" features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feature_space = \"wordnet\"\n\nfile_name = os.path.join(directory, \"features\", f\"{feature_space}.hdf\")\nX_train = load_hdf5_array(file_name, key=\"X_train\")\nX_test = load_hdf5_array(file_name, key=\"X_test\")\n\nprint(\"(n_samples_train, n_features) =\", X_train.shape)\nprint(\"(n_samples_test, n_features) =\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the cross-validation scheme\n----------------------------------\n\nWe define the same leave-one-run-out cross-validation split as in the\nprevious example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import check_cv\nfrom voxelwise_tutorials.utils import generate_leave_one_run_out\n\n# indice of first sample of each run\nrun_onsets = load_hdf5_array(file_name, key=\"run_onsets\")\nprint(run_onsets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a cross-validation splitter, compatible with ``scikit-learn`` API.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples_train = X_train.shape[0]\ncv = generate_leave_one_run_out(n_samples_train, run_onsets)\ncv = check_cv(cv)  # copy the cross-validation splitter into a reusable list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the model\n----------------\n\nWe define the same model as in the previous example. See the previous\nexample for more details about the model definition.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom voxelwise_tutorials.delayer import Delayer\nfrom himalaya.kernel_ridge import KernelRidgeCV\nfrom himalaya.backend import set_backend\nbackend = set_backend(\"torch_cuda\", on_error=\"warn\")\n\nX_train = X_train.astype(\"float32\")\nX_test = X_test.astype(\"float32\")\n\nalphas = np.logspace(1, 20, 20)\n\npipeline = make_pipeline(\n    StandardScaler(with_mean=True, with_std=False),\n    Delayer(delays=[1, 2, 3, 4]),\n    KernelRidgeCV(\n        alphas=alphas, cv=cv,\n        solver_params=dict(n_targets_batch=500, n_alphas_batch=5,\n                           n_targets_batch_refit=100)),\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn import set_config\nset_config(display='diagram')  # requires scikit-learn 0.23\npipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the model\n-------------\n\nWe fit on the train set, and score on the test set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline.fit(X_train, Y_train)\n\nscores = pipeline.score(X_test, Y_test)\nscores = backend.to_numpy(scores)\nprint(\"(n_voxels,) =\", scores.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare with a model without delays\n-----------------------------------\n\nWe define here another model without feature delays (i.e. no ``Delayer``).\nBecause the BOLD signal is inherently slow due to the dynamics of\nneuro-vascular coupling, this model is unlikely to perform well.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline_no_delay = make_pipeline(\n    StandardScaler(with_mean=True, with_std=False),\n    KernelRidgeCV(\n        alphas=alphas, cv=cv,\n        solver_params=dict(n_targets_batch=500, n_alphas_batch=5,\n                           n_targets_batch_refit=100)),\n)\npipeline_no_delay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We fit and score the model as the previous one.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline_no_delay.fit(X_train, Y_train)\nscores_nodelay = pipeline_no_delay.score(X_test, Y_test)\nscores_nodelay = backend.to_numpy(scores_nodelay)\nprint(\"(n_voxels,) =\", scores_nodelay.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we plot the comparison of model performances with a 2D histogram.\nAll ~70k voxels are represented in this histogram, where the diagonal\ncorresponds to identical performance for both models. A distibution deviating\nfrom the diagonal means that one model has better predictive performances\nthan the other.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom voxelwise_tutorials.viz import plot_hist2d\n\nax = plot_hist2d(scores_nodelay, scores)\nax.set(\n    title='Generalization R2 scores',\n    xlabel='model without delays',\n    ylabel='model with delays',\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the model with delays performs much better than the model without\ndelays. This can be seen in voxels with scores above 0. The distribution\nof scores below zero is not very informative, since it corresponds to voxels\nwith poor predictive performances anyway, and it only shows which model is\noverfitting the most.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the HRF\n-----------------\n\nWe just saw that delays are necessary to model BOLD responses. Here we show\nhow the fitted ridge regression weights follow the hemodynamic response\nfunction (HRF).\n\nFitting a kernel ridge regression results in a set of coefficients called the\n\"dual\" coefficients $w$. These coefficients differ from the \"primal\"\ncoefficients $\\beta$ obtained with a ridge regression, but the primal\ncoefficients can be computed from the dual coefficients using the training\nfeatures $X$:\n\n\\begin{align}\\beta = X^\\top w\\end{align}\n\nTo better visualize the HRF, we will refit a model with more delays, but only\non a selection of voxels to speed up the computations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# pick the 10 best voxels\nvoxel_selection = np.argsort(scores)[-10:]\n\n# define a pipeline with more delays\npipeline_more_delays = make_pipeline(\n    StandardScaler(with_mean=True, with_std=False),\n    Delayer(delays=[0, 1, 2, 3, 4, 5, 6]),\n    KernelRidgeCV(\n        alphas=alphas, cv=cv,\n        solver_params=dict(n_targets_batch=500, n_alphas_batch=5,\n                           n_targets_batch_refit=100)),\n)\n\npipeline_more_delays.fit(X_train, Y_train[:, voxel_selection])\n\n# get the (primal) ridge regression coefficients\nprimal_coef = pipeline_more_delays[-1].get_primal_coef()\nprimal_coef = backend.to_numpy(primal_coef)\n\n# split the ridge coefficients per delays\ndelayer = pipeline_more_delays.named_steps['delayer']\nprimal_coef_per_delay = delayer.reshape_by_delays(primal_coef, axis=0)\nprint(\"(n_delays, n_features, n_voxels) =\", primal_coef_per_delay.shape)\n\n# select the feature with the largest coefficients for each voxel\nfeature_selection = np.argmax(np.sum(np.abs(primal_coef_per_delay), axis=0),\n                              axis=0)\nprimal_coef_selection = primal_coef_per_delay[:, feature_selection,\n                                              np.arange(len(voxel_selection))]\n\nplt.plot(delayer.delays, primal_coef_selection)\nplt.xlabel('Delays')\nplt.xticks(delayer.delays)\nplt.ylabel('Ridge coefficients')\nplt.title(f'Largest feature for the {len(voxel_selection)} best voxels')\nplt.axhline(0, color='k', linewidth=0.5)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the hemodynamic response function (HRF) is captured in the model\nweights. Note that in this dataset, the brain responses are recorded every\ntwo seconds.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}