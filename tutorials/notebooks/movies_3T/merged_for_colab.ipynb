{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 00_load_colab.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Setup Google Colab\n",
    "\n",
    "\n",
    "In this script, we setup a Google Colab environment. This script will only work\n",
    "when run from `Google Colab <https://colab.research.google.com/>`_). You can\n",
    "skip it if you run the tutorials on your machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change runtime to use a GPU\n",
    "---------------------------\n",
    "\n",
    "This tutorial is much faster when a GPU is available to run the computations:\n",
    "\n",
    "(Menu) \"Runtime\" -> \"Change runtime type\" -> \"Hardware accelerator\" -> \"GPU\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount data with Google Drive\n",
    "----------------------------\n",
    "\n",
    "First, open the following Google Drive link:\n",
    "https://drive.google.com/drive/folders/1NuxO5_GHgDvjrL2FX5ohzAsvWpZuepIA\n",
    "\n",
    "Then, clic on the directory name (\"vim-5\"), and add a shortcut to your Drive\n",
    "(\"Add shortcut to Drive\").\n",
    "\n",
    "Then, mount Google Drive in Google Colab. To do so, run the following cell,\n",
    "and follow the instructions from Google to copy/paste the authorization code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the following command to check that Google Drive was\n",
    "correctly mounted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !ls 'drive' && echo SUCCESS || echo FAILURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, tell the voxelwise_tutorials package where the data is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['VOXELWISE_TUTORIALS_DATA'] = \"drive/MyDrive/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install package helper\n",
    "----------------------\n",
    "\n",
    "Finally, install the tutorial helper package, by uncommenting and running\n",
    "the install command.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pip install voxelwise_tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Colab, the install of pycortex might fail to locate the default\n",
    "pycortex filestore. You can fix it by uncommenting and running the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/gallantlab/pycortex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and changing the filestore path.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cortex\n",
    "filestore = \"/content/pycortex/filestore/\"\n",
    "cortex.options.config['basic']['filestore'] = filestore\n",
    "cortex.options.config['webgl']['colormaps'] = \"/content/pycortex/filestore/colormaps\"\n",
    "cortex.database.db = cortex.database.Database(filestore)\n",
    "cortex.db = cortex.database.db\n",
    "cortex.utils.db = cortex.database.db\n",
    "cortex.dataset.braindata.db = cortex.database.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 01_plot_explainable_variance.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Compute the explainable variance\n",
    "\n",
    "\n",
    "Before fitting voxelwise models to the fMRI responses, we can estimate the\n",
    "*explainable variance*. The explainable variance is the part of the fMRI\n",
    "responses that can be explained by the voxelwise modeling framework.\n",
    "\n",
    "Indeed, we can decompose the signal into a sum of two components, one component\n",
    "that is repeated if we repeat the same experiment, and one component that\n",
    "changes for each repeat. Because voxelwise modeling would use the same features\n",
    "for each repeat, it can only model the component that is common to all repeats.\n",
    "This shared component can be estimated by taking the mean over repeats of the\n",
    "same experiment. The variance of this shared component, that we call the\n",
    "explainable variance, is the upper bound of the voxelwise modeling\n",
    "performances. The explainable variance is also sometimes called the *noise\n",
    "ceiling*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path of the data directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from voxelwise_tutorials.io import get_data_home\n",
    "directory = os.path.join(get_data_home(), \"vim-5\")\n",
    "print(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modify to use another subject\n",
    "subject = \"S01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the explainable variance\n",
    "--------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from voxelwise_tutorials.io import load_hdf5_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the fMRI responses on the test set, which contains ten (10)\n",
    "repeats.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_name = os.path.join(directory, 'responses', f'{subject}_responses.hdf')\n",
    "Y_test = load_hdf5_array(file_name, key=\"Y_test\")\n",
    "print(\"(n_repeats, n_samples_test, n_voxels) =\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we compute the explainable variance per voxel.\n",
    "The variance of the signal is estimated by taking the average variance over\n",
    "repeats. The variance of the component shared across repeats is estimated by\n",
    "taking the variance of the average response. Then, we compute the\n",
    "explainable variance by dividing these two quantities.\n",
    "Finally, a correction can be applied to account for small numbers of repeat\n",
    "(through the parameter ``bias_correction``).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from voxelwise_tutorials.utils import explainable_variance\n",
    "ev = explainable_variance(Y_test, bias_correction=False)\n",
    "print(\"(n_voxels,) =\", ev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the explainable variance, we can plot the time-courses\n",
    "of a voxel with large explainable variance...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "voxel_1 = np.argmax(ev)\n",
    "time = np.arange(Y_test.shape[1]) * 2  # one time point every 2 seconds\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(time, Y_test[:, :, voxel_1].T, color='C0', alpha=0.5)\n",
    "plt.plot(time, Y_test[:, :, voxel_1].mean(0), color='C1', label='average')\n",
    "plt.xlabel(\"Time (sec)\")\n",
    "plt.title(\"Voxel with large explainable variance (%.2f)\" % ev[voxel_1])\n",
    "plt.yticks([])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and of a voxel with low explainable variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voxel_2 = np.argmin(ev)\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(time, Y_test[:, :, voxel_2].T, color='C0', alpha=0.5)\n",
    "plt.plot(time, Y_test[:, :, voxel_2].mean(0), color='C1', label='average')\n",
    "plt.xlabel(\"Time (sec)\")\n",
    "plt.title(\"Voxel with low explainable variance (%.2f)\" % ev[voxel_2])\n",
    "plt.yticks([])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the distribution of explainable variance over voxels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(ev, bins=np.linspace(0, 1, 100), log=True, histtype='step')\n",
    "plt.xlabel(\"Explainable variance\")\n",
    "plt.ylabel(\"Number of voxels\")\n",
    "plt.title('Histogram of explainable variance')\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most voxels have a rather low explainable variance, around 0.1\n",
    "(when not using the bias correction). This is expected, since most voxels are\n",
    "not directly driven by a visual stimulus, and their activity change over\n",
    "repeats. We also see that some voxels reach an explainable variance of 0.7,\n",
    "which is quite high. It means that these voxels consistently record the same\n",
    "activity across a repeated stimulus, and thus are good targets for encoding\n",
    "models. Of course, this set of explainable voxels changes from task to\n",
    "task, depending on what you are trying to model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map to subject flatmap\n",
    "----------------------\n",
    "\n",
    "To better understand the distribution of explainable variance, we map the\n",
    "values to the subject brain. This can be done with `pycortex\n",
    "<https://gallantlab.github.io/pycortex/>`_, which can create interactive 3D\n",
    "viewers to be displayed in any modern browser. ``pycortex`` can also display\n",
    "flattened maps of the cortical surface, to visualize the entire cortical\n",
    "surface at once.\n",
    "\n",
    "Here, we do not share the anatomical information of the subjects for privacy\n",
    "concerns. Instead, we provide two mappers:\n",
    "\n",
    "- to map the voxels to a (subject-specific) flatmap\n",
    "- to map the voxels to the Freesurfer average cortical surface (\"fsaverage\")\n",
    "\n",
    "The first mapper is 2D matrix of shape (n_pixels, n_voxels), that map each\n",
    "voxel to a set of pixel in a flatmap. The matrix is efficient stored using a\n",
    "``scipy`` sparse CSR matrix format. The function ``plot_flatmap_from_mapper``\n",
    "provides an example of how to use the mapper and visualize the flatmap.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from voxelwise_tutorials.viz import plot_flatmap_from_mapper\n",
    "\n",
    "mapper_file = os.path.join(directory, 'mappers', f'{subject}_mappers.hdf')\n",
    "plot_flatmap_from_mapper(ev, mapper_file, vmin=0, vmax=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure is a flatten map of the cortical surface. A number of regions of\n",
    "interest (ROIs) have been labeled to ease the interpretation. If you have\n",
    "never seen such a flatmap, we recommend taking a look at a `pycortex brain\n",
    "viewer <https://www.gallantlab.org/brainviewer/Deniz2019>`_, which displays\n",
    "the brain in 3D. In this viewer, press \"I\" to inflate the brain, \"F\" to\n",
    "flatten the surface, and \"R\" to reset the view (or use the ``surface/unfold``\n",
    "cursor on the right menu). Press \"H\" for a list of all keyboard shortcuts.\n",
    "This viewer should help you understand the correspondance between the flatten\n",
    "and the folded cortical surface of the brain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this flatmap, we can see that the explainable variance is mainly located\n",
    "in the visual cortex, in early visual regions like V1, V2, V3, or in\n",
    "higher-level regions like EBA, FFA or IPS. This was expected since this is a\n",
    "purely visual experiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map to \"fsaverage\"\n",
    "------------------\n",
    "\n",
    "The second mapper we provide maps the voxel data to a Freesurfer\n",
    "average surface (\"fsaverage\"), that can be used in ``pycortex``.\n",
    "First, let's download the \"fsaverage\" surface.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cortex\n",
    "\n",
    "surface = \"fsaverage\"\n",
    "\n",
    "if not hasattr(cortex.db, surface):\n",
    "    cortex.utils.download_subject(subject_id=surface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running the notebook on Colab, you might need to update the\n",
    "pycortex filestore as following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab  # noqa\n",
    "    in_colab = True\n",
    "except ImportError:\n",
    "    in_colab = False\n",
    "print(in_colab)\n",
    "\n",
    "if in_colab:\n",
    "    filestore = cortex.options.config['basic']['filestore']\n",
    "    cortex.database.db = cortex.database.Database(filestore)\n",
    "    cortex.db = cortex.database.db\n",
    "    cortex.utils.db = cortex.database.db\n",
    "    cortex.dataset.braindata.db = cortex.database.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the \"fsaverage\" mapper. The mapper is a matrix of shape\n",
    "(n_vertices, n_voxels), which maps each voxel to some vertices in the\n",
    "fsaverage surface. It is stored as a sparse CSR matrix. The mapper is applied\n",
    "with a dot product ``@`` (equivalent to ``np.dot``).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from voxelwise_tutorials.io import load_hdf5_sparse_array\n",
    "voxel_to_fsaverage = load_hdf5_sparse_array(mapper_file,\n",
    "                                            key='voxel_to_fsaverage')\n",
    "ev_projected = voxel_to_fsaverage @ ev\n",
    "print(\"(n_vertices,) =\", ev_projected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a ``Vertex`` object in ``pycortex``, containing the\n",
    "projected data. This object can be used either in a ``pycortex`` interactive\n",
    "3D viewer, or in a ``matplotlib`` figure showing only the flatmap.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vertex = cortex.Vertex(ev_projected, surface, vmin=0, vmax=0.7, cmap='inferno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start an interactive 3D viewer in the browser, use the following function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    cortex.webshow(vertex, open_browser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, to plot a flatmap in a ``matplotlib`` figure, use the\n",
    "`quickshow` function.\n",
    "\n",
    "(This function requires Inkscape to be installed. The rest of the tutorial\n",
    "does not use this function, so feel free to ignore.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cortex.testing_utils import has_installed\n",
    "\n",
    "if has_installed(\"inkscape\"):\n",
    "    fig = cortex.quickshow(vertex, colorbar_location='right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 02_plot_wordnet_model.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Fit a ridge model with wordnet features\n",
    "\n",
    "\n",
    "In this example, we model the fMRI responses with semantic \"wordnet\" features,\n",
    "manually annotated on each frame of the movie stimulus. The model is a\n",
    "regularized linear regression model, known as ridge regression. Since this\n",
    "model is used to predict brain activity from the stimulus, it is called a\n",
    "(voxelwise) encoding model.\n",
    "\n",
    "This example reproduces part of the analysis described in Huth et al (2012)\n",
    "[1]_. See this publication for more details about the experiment, the wordnet\n",
    "features, along with more results and more discussions.\n",
    "\n",
    "*Wordnet features:* The features used in this example are semantic labels\n",
    "manually annotated on each frame of the movie stimulus. The semantic labels\n",
    "include nouns (such as \"woman\", \"car\", or \"building\") and verbs (such as\n",
    "\"talking\", \"touching\", or \"walking\"), for a total of 1705 distinct category\n",
    "labels. To interpret our model, labels can be organized in a graph of semantic\n",
    "relashionship based on the `Wordnet <https://wordnet.princeton.edu/>`_ dataset.\n",
    "\n",
    "*Summary:* We first concatenate the features with multiple temporal delays, to\n",
    "account for the slow hemodynamic response. We then fit a predictive model of\n",
    "BOLD activity, using a  linear regression that weighs each delayed feature\n",
    "differently. The linear regression is regularized to improve robustness to\n",
    "correlated features and to improve generalization. The optimal regularization\n",
    "hyperparameter is selected over a grid-search with cross-validation. Finally,\n",
    "the model generalization performance is evaluated on a held-out test set,\n",
    "comparing the model predictions with the corresponding ground-truth fMRI\n",
    "responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path of the data directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from voxelwise_tutorials.io import get_data_home\n",
    "directory = os.path.join(get_data_home(), \"vim-5\")\n",
    "print(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modify to use another subject\n",
    "subject = \"S01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data\n",
    "-------------\n",
    "\n",
    "We first load the fMRI responses. These responses have been preprocessed as\n",
    "decribed in [1]_. The data is separated into a training set ``Y_train`` and a\n",
    "testing set ``Y_test``. The training set is used for fitting models, and\n",
    "selecting the best models and hyperparameters. The testing set is later used\n",
    "to estimate the generalization performances of the selected model. The\n",
    "testing set contains multiple repetitions of the same experiment, to estimate\n",
    "an upper bound of the model performances (cf. previous example).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from voxelwise_tutorials.io import load_hdf5_array\n",
    "\n",
    "file_name = os.path.join(directory, \"responses\", f\"{subject}_responses.hdf\")\n",
    "Y_train = load_hdf5_array(file_name, key=\"Y_train\")\n",
    "Y_test = load_hdf5_array(file_name, key=\"Y_test\")\n",
    "\n",
    "print(\"(n_samples_train, n_voxels) =\", Y_train.shape)\n",
    "print(\"(n_repeats, n_samples_test, n_voxels) =\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we repeat an experiment multiple times, part of the fMRI responses might\n",
    "change. However the modeling features do not change over the repeats, so the\n",
    "voxelwise encoding model predicts the same signal for each repeat. To have an\n",
    "upper bound of the model performances, we keep only the repeatable part of\n",
    "the signal by averaging the test repeats. It means that the prediction\n",
    "$R^2$ scores will be relative to the explainable variance (cf. previous\n",
    "example).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_test = Y_test.mean(0)\n",
    "\n",
    "print(\"(n_samples_test, n_voxels) =\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fill potential NaN (not-a-number) values with zeros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train = np.nan_to_num(Y_train)\n",
    "Y_test = np.nan_to_num(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the semantic \"wordnet\" features, extracted from the stimulus at\n",
    "each time point. The features corresponding to the training set are noted\n",
    "``X_train``, and the features corresponding to the testing set are noted\n",
    "``X_test``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_space = \"wordnet\"\n",
    "\n",
    "file_name = os.path.join(directory, \"features\", f\"{feature_space}.hdf\")\n",
    "X_train = load_hdf5_array(file_name, key=\"X_train\")\n",
    "X_test = load_hdf5_array(file_name, key=\"X_test\")\n",
    "\n",
    "print(\"(n_samples_train, n_features) =\", X_train.shape)\n",
    "print(\"(n_samples_test, n_features) =\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the cross-validation scheme\n",
    "----------------------------------\n",
    "\n",
    "To select the best hyperparameter through cross-validation, we must define a\n",
    "train-validation splitting scheme. Since fMRI time-series are autocorrelated\n",
    "in time, we should preserve as much as possible the time blocks.\n",
    "In other words, since consecutive time samples are correlated, we should not\n",
    "put one time sample in the training set and the immediately following time\n",
    "sample in the validation set. Thus, we define here a leave-one-run-out\n",
    "cross-validation split, which preserves each recording run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import check_cv\n",
    "from voxelwise_tutorials.utils import generate_leave_one_run_out\n",
    "\n",
    "# indice of first sample of each run\n",
    "run_onsets = load_hdf5_array(file_name, key=\"run_onsets\")\n",
    "print(run_onsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a cross-validation splitter, compatible with ``scikit-learn`` API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples_train = X_train.shape[0]\n",
    "cv = generate_leave_one_run_out(n_samples_train, run_onsets)\n",
    "cv = check_cv(cv)  # copy the cross-validation splitter into a reusable list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model\n",
    "----------------\n",
    "\n",
    "Now, let's define the model pipeline.\n",
    "\n",
    "We first center the features, since we will not use an intercept. Indeed, the\n",
    "mean value in fMRI recording is non-informative, so each run is detrended and\n",
    "demeaned independently, and we do not need to predict an intercept value in\n",
    "the linear model.\n",
    "\n",
    "However, we prefer not to normalize by the standard deviation of each\n",
    "feature. Indeed, if the features are extracted in a consistent way from the\n",
    "stimulus, their relative scale is meaningful. Normalizing them independently\n",
    "from each other would remove this meaning. Moreover, the wordnet features are\n",
    "one-hot-encoded, which means that each feature is either present (1) or not\n",
    "present (0) in each sample. Normalizing one-hot-encoded features is not\n",
    "recommended, since it would scale disproportionately the infrequent features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=True, with_std=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we concatenate the features with multiple delays to account for the\n",
    "hemodynamic response. Indeed, the BOLD signal recorded in fMRI experiments is\n",
    "delayed in time with respect to the stimulus. With different delayed versions\n",
    "of the features, the linear regression model will weight each delayed feature\n",
    "with a different weight, to maximize the predictions. With a sample every 2\n",
    "seconds, we typically use 4 delays [1, 2, 3, 4] to cover the most part of the\n",
    "hemodynamic response peak. In the next example, we further describe this\n",
    "hemodynamic response estimation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from voxelwise_tutorials.delayer import Delayer\n",
    "delayer = Delayer(delays=[1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use a ridge regression model. Ridge regression is a linear\n",
    "regression with a L2 regularization. The L2 regularizatin improves robustness\n",
    "to correlated features and improves generalization. However, the L2\n",
    "regularization is controled by a hyperparameter ``alpha`` that needs to be\n",
    "tuned. This regularization hyperparameter is usually selected over a grid\n",
    "search with cross-validation, selecting the hyperparameter that maximizes the\n",
    "predictive performances on the validation set. More details about\n",
    "cross-validation can be found in the `scikit-learn documentation\n",
    "<https://scikit-learn.org/stable/modules/cross_validation.html>`_.\n",
    "\n",
    "For computational reasons, when the number of features is larger than the\n",
    "number of samples, it is more efficient to solve a ridge regression using the\n",
    "(equivalent) dual formulation [2]_. This dual formulation is equivalent to\n",
    "kernel ridge regression with a linear kernel. Here, we have 3600 training\n",
    "samples, and 1705 * 4 = 6820 features (we multiply by 4 since we use 4 time\n",
    "delays), therefore it is more efficient to use kernel ridge regression.\n",
    "\n",
    "With one target, we could directly use the pipeline in ``scikit-learn``'s\n",
    "``GridSearchCV``, to select the optimal regularization hyperparameter\n",
    "(``alpha``) over cross-validation. However, ``GridSearchCV`` can only\n",
    "optimize one score. Thus, in the multiple-target case, ``GridSearchCV`` can\n",
    "only optimize (for example) the mean score over targets. Here, we want to\n",
    "find a different optimal hyperparameter per target/voxel, so we use the\n",
    "package `himalaya <https://github.com/gallantlab/himalaya>`_ which implements\n",
    "a ``scikit-learn`` compatible estimator ``KernelRidgeCV``, with\n",
    "hyperparameter selection independently on each target.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from himalaya.kernel_ridge import KernelRidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, ``himalaya`` implements different computational backends,\n",
    "including two backends that use GPU for faster computations. The two\n",
    "available GPU backends are \"torch_cuda\" and \"cupy\". (Each backend is only\n",
    "available if you installed the corresponding package with CUDA enabled. Check\n",
    "the ``pytorch``/``cupy`` documentation for install instructions.)\n",
    "\n",
    "Here we use the \"torch_cuda\" backend, but if the import fails we continue\n",
    "with the default \"numpy\" backend. The \"numpy\" backend is expected to be\n",
    "slower since it only uses the CPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from himalaya.backend import set_backend\n",
    "backend = set_backend(\"torch_cuda\", on_error=\"warn\")\n",
    "print(backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up model fitting on GPU, we use single precision float numbers.\n",
    "(This step probably does not change significantly the performances on non-GPU\n",
    "backends.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the scale of the regularization hyperparameter ``alpha`` is unknown, we\n",
    "use a large logarithmic range, and we will check after the fit that best\n",
    "hyperparameters are not all on one range edge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(1, 20, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also indicate some batch sizes to limit the GPU memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kernel_ridge_cv = KernelRidgeCV(\n",
    "    alphas=alphas, cv=cv,\n",
    "    solver_params=dict(n_targets_batch=500, n_alphas_batch=5,\n",
    "                       n_targets_batch_refit=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use a ``scikit-learn`` ``Pipeline`` to link the different steps\n",
    "together. A ``Pipeline`` can be used as a regular estimator, calling\n",
    "``pipeline.fit``, ``pipeline.predict``, etc. Using a ``Pipeline`` can be\n",
    "useful to clarify the different steps, avoid cross-validation mistakes, or\n",
    "automatically cache intermediate results. See the ``scikit-learn``\n",
    "`documentation <https://scikit-learn.org/stable/modules/compose.html>`_ for\n",
    "more information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "pipeline = make_pipeline(\n",
    "    scaler,\n",
    "    delayer,\n",
    "    kernel_ridge_cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the ``scikit-learn`` pipeline with an HTML diagram.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')  # requires scikit-learn 0.23\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model\n",
    "-------------\n",
    "\n",
    "We fit on the train set..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..and score on the test set. Here the scores are the $R^2$ scores, with\n",
    "values in $]-\\infty, 1]$. A value of $1$ means the predictions\n",
    "are perfect.\n",
    "\n",
    "Note that since ``himalaya`` is specifically implementing multiple targets\n",
    "models, the ``score`` method differs from ``scikit-learn`` API and returns\n",
    "one score per target/voxel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = pipeline.score(X_test, Y_test)\n",
    "print(\"(n_voxels,) =\", scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we fit the model on GPU, scores are returned on GPU using an array object\n",
    "specfic to the backend we used (such as a ``torch.Tensor``). Thus, we need to\n",
    "move them into ``numpy`` arrays on CPU, to be able to use them for example in\n",
    "a ``matplotlib`` figure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = backend.to_numpy(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the model performances\n",
    "---------------------------\n",
    "\n",
    "To visualize the model performances, we can plot them on a flattened\n",
    "surface of the brain, using a mapper that is specific to the subject brain.\n",
    "(Check previous example to see how to use the mapper to Freesurfer average\n",
    "surface.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from voxelwise_tutorials.viz import plot_flatmap_from_mapper\n",
    "\n",
    "mapper_file = os.path.join(directory, \"mappers\", f\"{subject}_mappers.hdf\")\n",
    "ax = plot_flatmap_from_mapper(scores, mapper_file, vmin=0, vmax=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the \"wordnet\" features successfully predict a part of the\n",
    "brain activity, with $R^2$ scores as high as 0.4. Note that these\n",
    "scores are generalization scores, since they are computed on a test set not\n",
    "seen during the mode fitting. Since we fitted a model independently on each\n",
    "voxel, we can show the generalization performances at the maximal resolution,\n",
    "the voxel.\n",
    "\n",
    "The best performances are located in visual semantic areas like EBA, or FFA.\n",
    "This is expected since the wordnet features encode semantic information about\n",
    "the visual stimulus. For more discussions about these results, we refer the\n",
    "reader to the original publication [1]_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the selected hyperparameters\n",
    "---------------------------------\n",
    "\n",
    "Since the scale of alphas is unknown, we plot the optimal alphas selected by\n",
    "the solver over cross-validation. This plot is helpful to refine the alpha\n",
    "grid if the range is too small or too large.\n",
    "\n",
    "Note that some voxels might be at the maximum regularization value in the\n",
    "grid search. These are voxels where the model has no predictive power, thus\n",
    "the optimal regularization parameter is large to lead to a prediction equal\n",
    "to zero. We do not need to extend the alpha range for these voxels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from himalaya.viz import plot_alphas_diagnostic\n",
    "best_alphas = backend.to_numpy(pipeline[-1].best_alphas_)\n",
    "plot_alphas_diagnostic(best_alphas=best_alphas, alphas=alphas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the regression coefficients\n",
    "-------------------------------------\n",
    "\n",
    "Here, we go back to the main model on all voxels. Since our model is linear,\n",
    "we can use the (primal) regression coefficients to interpret the model. The\n",
    "basic intuition is that the model will use larger coefficients on features\n",
    "that have more predictive power.\n",
    "\n",
    "Since we know the meaning of each feature, we can interpret the large\n",
    "regression coefficients. In the case of wordnet features, we can even build\n",
    "a graph that represents the features linked by a semantic relationship.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first get the (primal) ridge regression coefficients from the fitted\n",
    "model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primal_coef = pipeline[-1].get_primal_coef()\n",
    "primal_coef = backend.to_numpy(primal_coef)\n",
    "print(\"(n_delays * n_features, n_voxels) =\", primal_coef.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are only interested in the voxels with good generalization\n",
    "performances. We select an arbitrary threshold of 0.05 (R^2 score).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primal_coef_selection = primal_coef[:, scores > 0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we aggregate the coefficients across the different delays.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split the ridge coefficients per delays\n",
    "delayer = pipeline.named_steps['delayer']\n",
    "primal_coef_per_delay = delayer.reshape_by_delays(primal_coef_selection,\n",
    "                                                  axis=0)\n",
    "print(\"(n_delays, n_features, n_voxels) =\", primal_coef_per_delay.shape)\n",
    "\n",
    "# average over delays\n",
    "average_coef = np.mean(primal_coef_per_delay, axis=0)\n",
    "print(\"(n_features, n_voxels) =\", average_coef.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after averaging over delays, the coefficient matrix is still too large\n",
    "to understand it. Therefore, we use principal component analysis (PCA) to\n",
    "reduce the dimensionality of the matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(average_coef.T)\n",
    "components = pca.components_\n",
    "print(\"(n_components, n_features) =\", components.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the ratio of explained variance by each principal component.\n",
    "We see that the first four components already explain a large part of the\n",
    "coefficients variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"PCA explained variance =\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to [1]_, we correct the coefficients of features linked by a\n",
    "semantic relationship. Indeed, in the wordnet features, if a clip was labeled\n",
    "with `wolf`, the authors automatically added the categories `canine`,\n",
    "`carnivore`, `placental mammal`, `mamma`, `vertebrate`, `chordate`,\n",
    "`organism`, and `whole`. The authors thus argue that the same correction\n",
    "needs to be done on the coefficients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from voxelwise_tutorials.wordnet import load_wordnet\n",
    "from voxelwise_tutorials.wordnet import correct_coefficients\n",
    "_, wordnet_categories = load_wordnet(directory=directory)\n",
    "components = correct_coefficients(components.T, wordnet_categories).T\n",
    "components -= components.mean(axis=1)[:, None]\n",
    "components /= components.std(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot the first principal component on the wordnet graph. In such\n",
    "graph, links indicate \"is a\" relationships (e.g. an `athlete` \"is a\"\n",
    "`person`). Each marker represents a single noun (circle) or verb (square).\n",
    "The area of each marker indicates the principal component magnitude, and the\n",
    "color indicates the sign (red is positive, blue is negative).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from voxelwise_tutorials.wordnet import plot_wordnet_graph\n",
    "from voxelwise_tutorials.wordnet import apply_cmap\n",
    "\n",
    "first_component = components[0]\n",
    "node_sizes = np.abs(first_component)\n",
    "node_colors = apply_cmap(first_component, vmin=-2, vmax=2, cmap='coolwarm',\n",
    "                         n_colors=2)\n",
    "\n",
    "plot_wordnet_graph(node_colors=node_colors, node_sizes=node_sizes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the authors of [1]_, \"this principal component distinguishes\n",
    "between categories with high stimulus energy (e.g. moving objects like\n",
    "`person` and `vehicle`) and those with low stimulus energy (e.g. stationary\n",
    "objects like `sky` and `city`)\".\n",
    "\n",
    "Our result is slightly different than in [1]_, since we only use one subject,\n",
    "and the voxel selection is slightly different. We also use a different\n",
    "regularization parameter in each voxels, while in [1]_ all voxels use the\n",
    "same regularization parameter. Here, we do not aim at reproducing exactly the\n",
    "results in [1]_, but we rather describe the general approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To project the principal component on the cortical surface, we first need to\n",
    "transform the primal weights of all voxels using the fitted PCA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split the ridge coefficients per delays\n",
    "primal_coef_per_delay = delayer.reshape_by_delays(primal_coef, axis=0)\n",
    "print(\"(n_delays, n_features, n_voxels) =\", primal_coef_per_delay.shape)\n",
    "del primal_coef\n",
    "\n",
    "# average over delays\n",
    "average_coef = np.mean(primal_coef_per_delay, axis=0)\n",
    "print(\"(n_features, n_voxels) =\", average_coef.shape)\n",
    "del primal_coef_per_delay\n",
    "\n",
    "# transform with the fitted PCA\n",
    "average_coef_transformed = pca.transform(average_coef.T).T\n",
    "print(\"(n_components, n_voxels) =\", average_coef_transformed.shape)\n",
    "del average_coef\n",
    "\n",
    "# We make sure vmin = -vmax, so that the colormap is centered on 0.\n",
    "vmax = np.percentile(np.abs(average_coef_transformed), 99.9)\n",
    "\n",
    "# plot the primal weights projected on the first principal component.\n",
    "ax = plot_flatmap_from_mapper(average_coef_transformed[0], mapper_file,\n",
    "                              vmin=-vmax, vmax=vmax, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This flatmap shows in which brain regions the model has the largest\n",
    "projection on the first component. Again, this result is different from the\n",
    "one in [1]_, and should only be considered as reproducing the general\n",
    "approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [1]_, we also plot the next three principal components on the\n",
    "wordnet graph, mapping the three vectors to RGB colors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from voxelwise_tutorials.wordnet import scale_to_rgb_cube\n",
    "\n",
    "next_three_components = components[1:4].T\n",
    "node_sizes = np.linalg.norm(next_three_components, axis=1)\n",
    "node_colors = scale_to_rgb_cube(next_three_components)\n",
    "print(\"(n_nodes, n_channels) =\", node_colors.shape)\n",
    "\n",
    "plot_wordnet_graph(node_colors=node_colors, node_sizes=node_sizes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the authors of [1]_, \"this graph shows that categories thought\n",
    "to be semantically related (e.g. athletes and walking) are represented\n",
    "similarly in the brain\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we project these principal components on the cortical surface.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from voxelwise_tutorials.viz import plot_3d_flatmap_from_mapper\n",
    "\n",
    "voxel_colors = scale_to_rgb_cube(average_coef_transformed[1:4].T, clip=3).T\n",
    "print(\"(n_channels, n_voxels) =\", voxel_colors.shape)\n",
    "\n",
    "ax = plot_3d_flatmap_from_mapper(voxel_colors[0], voxel_colors[1],\n",
    "                                 voxel_colors[2], mapper_file=mapper_file,\n",
    "                                 vmin=0, vmax=1, vmin2=0, vmax2=1, vmin3=0,\n",
    "                                 vmax3=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our results are different from the ones in [1]_, for the same reasons\n",
    "mentioned earlier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------\n",
    "\n",
    ".. [1] Huth, A. G., Nishimoto, S., Vu, A. T., & Gallant, J. L. (2012).\n",
    "   A continuous semantic space describes the representation of thousands of\n",
    "   object and action categories across the human brain. Neuron, 76(6),\n",
    "   1210-1224.\n",
    "\n",
    ".. [2] Saunders, C., Gammerman, A., & Vovk, V. (1998).\n",
    "   Ridge regression learning algorithm in dual variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 03_plot_hemodynamic_response.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Visualize the hemodynamic response\n",
    "\n",
    "\n",
    "In this example, we describe how the hemodynamic response is estimated in the\n",
    "previous model. We fit the same ridge model as in the previous example, and\n",
    "further describe the need to delay the features in time.\n",
    "\n",
    "As explained in previous example, the BOLD signal recorded in fMRI experiments\n",
    "is delayed in time with respect to the stimulus. With different delayed\n",
    "versions of the features, the linear regression model weight each delayed\n",
    "feature with a different weight, to maximize the predictions. With a sample\n",
    "every 2 seconds, we typically use 4 delays [1, 2, 3, 4] to cover the most part\n",
    "of the hemodynamic response peak.\n",
    "\n",
    "In this example, we show the descrease in prediction performances when using no\n",
    "delays. We also show how to visualize the estimated hemodynamic response\n",
    "function (HRF) using more delays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path of the data directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from voxelwise_tutorials.io import get_data_home\n",
    "directory = os.path.join(get_data_home(), \"vim-5\")\n",
    "print(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modify to use another subject\n",
    "subject = \"S01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data\n",
    "-------------\n",
    "\n",
    "We first load the fMRI responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from voxelwise_tutorials.io import load_hdf5_array\n",
    "\n",
    "file_name = os.path.join(directory, \"responses\", f\"{subject}_responses.hdf\")\n",
    "Y_train = load_hdf5_array(file_name, key=\"Y_train\")\n",
    "Y_test = load_hdf5_array(file_name, key=\"Y_test\")\n",
    "\n",
    "print(\"(n_samples_train, n_voxels) =\", Y_train.shape)\n",
    "print(\"(n_repeats, n_samples_test, n_voxels) =\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We average the test repeats, to remove the non-repeatable part of fMRI\n",
    "responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_test = Y_test.mean(0)\n",
    "\n",
    "print(\"(n_samples_test, n_voxels) =\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fill potential NaN (not-a-number) values with zeros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train = np.nan_to_num(Y_train)\n",
    "Y_test = np.nan_to_num(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the semantic \"wordnet\" features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_space = \"wordnet\"\n",
    "\n",
    "file_name = os.path.join(directory, \"features\", f\"{feature_space}.hdf\")\n",
    "X_train = load_hdf5_array(file_name, key=\"X_train\")\n",
    "X_test = load_hdf5_array(file_name, key=\"X_test\")\n",
    "\n",
    "print(\"(n_samples_train, n_features) =\", X_train.shape)\n",
    "print(\"(n_samples_test, n_features) =\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the cross-validation scheme\n",
    "----------------------------------\n",
    "\n",
    "We define the same leave-one-run-out cross-validation split as in the\n",
    "previous example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import check_cv\n",
    "from voxelwise_tutorials.utils import generate_leave_one_run_out\n",
    "\n",
    "# indice of first sample of each run\n",
    "run_onsets = load_hdf5_array(file_name, key=\"run_onsets\")\n",
    "print(run_onsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a cross-validation splitter, compatible with ``scikit-learn`` API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples_train = X_train.shape[0]\n",
    "cv = generate_leave_one_run_out(n_samples_train, run_onsets)\n",
    "cv = check_cv(cv)  # copy the cross-validation splitter into a reusable list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model\n",
    "----------------\n",
    "\n",
    "We define the same model as in the previous example. See the previous\n",
    "example for more details about the model definition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from voxelwise_tutorials.delayer import Delayer\n",
    "from himalaya.kernel_ridge import KernelRidgeCV\n",
    "from himalaya.backend import set_backend\n",
    "backend = set_backend(\"torch_cuda\", on_error=\"warn\")\n",
    "\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "\n",
    "alphas = np.logspace(1, 20, 20)\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(with_mean=True, with_std=False),\n",
    "    Delayer(delays=[1, 2, 3, 4]),\n",
    "    KernelRidgeCV(\n",
    "        alphas=alphas, cv=cv,\n",
    "        solver_params=dict(n_targets_batch=500, n_alphas_batch=5,\n",
    "                           n_targets_batch_refit=100)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')  # requires scikit-learn 0.23\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model\n",
    "-------------\n",
    "\n",
    "We fit on the train set, and score on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, Y_train)\n",
    "\n",
    "scores = pipeline.score(X_test, Y_test)\n",
    "scores = backend.to_numpy(scores)\n",
    "print(\"(n_voxels,) =\", scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with a model without delays\n",
    "-----------------------------------\n",
    "\n",
    "We define here another model without feature delays (i.e. no ``Delayer``).\n",
    "Because the BOLD signal is inherently slow due to the dynamics of\n",
    "neuro-vascular coupling, this model is unlikely to perform well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline_no_delay = make_pipeline(\n",
    "    StandardScaler(with_mean=True, with_std=False),\n",
    "    KernelRidgeCV(\n",
    "        alphas=alphas, cv=cv,\n",
    "        solver_params=dict(n_targets_batch=500, n_alphas_batch=5,\n",
    "                           n_targets_batch_refit=100)),\n",
    ")\n",
    "pipeline_no_delay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit and score the model as the previous one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline_no_delay.fit(X_train, Y_train)\n",
    "scores_nodelay = pipeline_no_delay.score(X_test, Y_test)\n",
    "scores_nodelay = backend.to_numpy(scores_nodelay)\n",
    "print(\"(n_voxels,) =\", scores_nodelay.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we plot the comparison of model performances with a 2D histogram.\n",
    "All ~70k voxels are represented in this histogram, where the diagonal\n",
    "corresponds to identical performance for both models. A distibution deviating\n",
    "from the diagonal means that one model has better predictive performances\n",
    "than the other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from voxelwise_tutorials.viz import plot_hist2d\n",
    "\n",
    "ax = plot_hist2d(scores_nodelay, scores)\n",
    "ax.set(\n",
    "    title='Generalization R2 scores',\n",
    "    xlabel='model without delays',\n",
    "    ylabel='model with delays',\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model with delays performs much better than the model without\n",
    "delays. This can be seen in voxels with scores above 0. The distribution\n",
    "of scores below zero is not very informative, since it corresponds to voxels\n",
    "with poor predictive performances anyway, and it only shows which model is\n",
    "overfitting the most.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the HRF\n",
    "-----------------\n",
    "\n",
    "We just saw that delays are necessary to model BOLD responses. Here we show\n",
    "how the fitted ridge regression weights follow the hemodynamic response\n",
    "function (HRF).\n",
    "\n",
    "Fitting a kernel ridge regression results in a set of coefficients called the\n",
    "\"dual\" coefficients $w$. These coefficients differ from the \"primal\"\n",
    "coefficients $\\beta$ obtained with a ridge regression, but the primal\n",
    "coefficients can be computed from the dual coefficients using the training\n",
    "features $X$:\n",
    "\n",
    "\\begin{align}\\beta = X^\\top w\\end{align}\n",
    "\n",
    "To better visualize the HRF, we will refit a model with more delays, but only\n",
    "on a selection of voxels to speed up the computations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pick the 10 best voxels\n",
    "voxel_selection = np.argsort(scores)[-10:]\n",
    "\n",
    "# define a pipeline with more delays\n",
    "pipeline_more_delays = make_pipeline(\n",
    "    StandardScaler(with_mean=True, with_std=False),\n",
    "    Delayer(delays=[0, 1, 2, 3, 4, 5, 6]),\n",
    "    KernelRidgeCV(\n",
    "        alphas=alphas, cv=cv,\n",
    "        solver_params=dict(n_targets_batch=500, n_alphas_batch=5,\n",
    "                           n_targets_batch_refit=100)),\n",
    ")\n",
    "\n",
    "pipeline_more_delays.fit(X_train, Y_train[:, voxel_selection])\n",
    "\n",
    "# get the (primal) ridge regression coefficients\n",
    "primal_coef = pipeline_more_delays[-1].get_primal_coef()\n",
    "primal_coef = backend.to_numpy(primal_coef)\n",
    "\n",
    "# split the ridge coefficients per delays\n",
    "delayer = pipeline_more_delays.named_steps['delayer']\n",
    "primal_coef_per_delay = delayer.reshape_by_delays(primal_coef, axis=0)\n",
    "print(\"(n_delays, n_features, n_voxels) =\", primal_coef_per_delay.shape)\n",
    "\n",
    "# select the feature with the largest coefficients for each voxel\n",
    "feature_selection = np.argmax(np.sum(np.abs(primal_coef_per_delay), axis=0),\n",
    "                              axis=0)\n",
    "primal_coef_selection = primal_coef_per_delay[:, feature_selection,\n",
    "                                              np.arange(len(voxel_selection))]\n",
    "\n",
    "plt.plot(delayer.delays, primal_coef_selection)\n",
    "plt.xlabel('Delays')\n",
    "plt.xticks(delayer.delays)\n",
    "plt.ylabel('Ridge coefficients')\n",
    "plt.title(f'Largest feature for the {len(voxel_selection)} best voxels')\n",
    "plt.axhline(0, color='k', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the hemodynamic response function (HRF) is captured in the model\n",
    "weights. Note that in this dataset, the brain responses are recorded every\n",
    "two seconds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 04_plot_motion_energy_model.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Fit a ridge model with motion energy features\n",
    "\n",
    "\n",
    "In this example, we model the fMRI responses with motion-energy features\n",
    "extracted from the movie stimulus. The model is a regularized linear regression\n",
    "model.\n",
    "\n",
    "This tutorial reproduces part of the analysis described in Nishimoto et al\n",
    "(2011) [1]_. See this publication for more details about the experiment, the\n",
    "motion-energy features, along with more results and more discussions.\n",
    "\n",
    "*Motion-energy features:* Motion-energy features result from filtering a video\n",
    "stimulus with spatio-temporal Gabor filters. A pyramid of filters is used to\n",
    "compute the motion-energy features at multiple spatial and temporal scales.\n",
    "Motion-energy features were introduced in [1]_.\n",
    "\n",
    "*Summary:* As in the previous example, we first concatenate the features with\n",
    "multiple delays, to account for the slow hemodynamic response. A linear\n",
    "regression model then weights each delayed feature with a different weight, to\n",
    "build a predictive model of BOLD activity. Again, the linear regression is\n",
    "regularized to improve robustness to correlated features and to improve\n",
    "generalization. The optimal regularization hyperparameter is selected\n",
    "independently on each voxel over a grid-search with cross-validation. Finally,\n",
    "the model generalization performance is evaluated on a held-out test set,\n",
    "comparing the model predictions with the ground-truth fMRI responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path of the data directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from voxelwise_tutorials.io import get_data_home\n",
    "directory = os.path.join(get_data_home(), \"vim-5\")\n",
    "print(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modify to use another subject\n",
    "subject = \"S01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data\n",
    "-------------\n",
    "\n",
    "We first load the fMRI responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from voxelwise_tutorials.io import load_hdf5_array\n",
    "\n",
    "file_name = os.path.join(directory, \"responses\", f\"{subject}_responses.hdf\")\n",
    "Y_train = load_hdf5_array(file_name, key=\"Y_train\")\n",
    "Y_test = load_hdf5_array(file_name, key=\"Y_test\")\n",
    "\n",
    "print(\"(n_samples_train, n_voxels) =\", Y_train.shape)\n",
    "print(\"(n_repeats, n_samples_test, n_voxels) =\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We average the test repeats, to remove the non-repeatable part of fMRI\n",
    "responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_test = Y_test.mean(0)\n",
    "\n",
    "print(\"(n_samples_test, n_voxels) =\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fill potential NaN (not-a-number) values with zeros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train = np.nan_to_num(Y_train)\n",
    "Y_test = np.nan_to_num(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the precomputed \"motion-energy\" features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_space = \"motion_energy\"\n",
    "file_name = os.path.join(directory, \"features\", f\"{feature_space}.hdf\")\n",
    "X_train = load_hdf5_array(file_name, key=\"X_train\")\n",
    "X_test = load_hdf5_array(file_name, key=\"X_test\")\n",
    "\n",
    "print(\"(n_samples_train, n_features) =\", X_train.shape)\n",
    "print(\"(n_samples_test, n_features) =\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the cross-validation scheme\n",
    "----------------------------------\n",
    "\n",
    "We define the same leave-one-run-out cross-validation split as in the\n",
    "previous example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import check_cv\n",
    "from voxelwise_tutorials.utils import generate_leave_one_run_out\n",
    "\n",
    "# indice of first sample of each run\n",
    "run_onsets = load_hdf5_array(file_name, key=\"run_onsets\")\n",
    "print(run_onsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a cross-validation splitter, compatible with ``scikit-learn`` API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples_train = X_train.shape[0]\n",
    "cv = generate_leave_one_run_out(n_samples_train, run_onsets)\n",
    "cv = check_cv(cv)  # copy the cross-validation splitter into a reusable list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model\n",
    "----------------\n",
    "\n",
    "We define the same model as in the previous example. See the previous\n",
    "example for more details about the model definition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from voxelwise_tutorials.delayer import Delayer\n",
    "from himalaya.kernel_ridge import KernelRidgeCV\n",
    "from himalaya.backend import set_backend\n",
    "backend = set_backend(\"torch_cuda\", on_error=\"warn\")\n",
    "\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "\n",
    "alphas = np.logspace(1, 20, 20)\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(with_mean=True, with_std=False),\n",
    "    Delayer(delays=[1, 2, 3, 4]),\n",
    "    KernelRidgeCV(\n",
    "        alphas=alphas, cv=cv,\n",
    "        solver_params=dict(n_targets_batch=500, n_alphas_batch=5,\n",
    "                           n_targets_batch_refit=100)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')  # requires scikit-learn 0.23\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model\n",
    "-------------\n",
    "\n",
    "We fit on the train set, and score on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, Y_train)\n",
    "\n",
    "scores_motion_energy = pipeline.score(X_test, Y_test)\n",
    "scores_motion_energy = backend.to_numpy(scores_motion_energy)\n",
    "\n",
    "print(\"(n_voxels,) =\", scores_motion_energy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the model performances\n",
    "---------------------------\n",
    "The performances are computed using the math:`R^2` scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from voxelwise_tutorials.viz import plot_flatmap_from_mapper\n",
    "\n",
    "mapper_file = os.path.join(directory, \"mappers\", f\"{subject}_mappers.hdf\")\n",
    "ax = plot_flatmap_from_mapper(scores_motion_energy, mapper_file, vmin=0,\n",
    "                              vmax=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The motion-energy features lead to large generalization scores in the\n",
    "early visual cortex (V1, V2, V3, ...). For more discussions about these\n",
    "results, we refer the reader to the original publication [1]_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with the wordnet model\n",
    "------------------------------\n",
    "\n",
    "Interestingly, the motion-energy model performs well in different brain\n",
    "regions than the semantic \"wordnet\" model fitted in the previous example. To\n",
    "compare the two models, we first need to fit again the wordnet model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_space = \"wordnet\"\n",
    "file_name = os.path.join(directory, \"features\", f\"{feature_space}.hdf\")\n",
    "X_train = load_hdf5_array(file_name, key=\"X_train\")\n",
    "X_test = load_hdf5_array(file_name, key=\"X_test\")\n",
    "\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create an unfitted copy of the pipeline with the ``clone`` function,\n",
    "or simply call fit again if we do not need to reuse the previous model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    from sklearn.base import clone\n",
    "    pipeline_wordnet = clone(pipeline)\n",
    "    pipeline_wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, Y_train)\n",
    "scores_wordnet = pipeline.score(X_test, Y_test)\n",
    "scores_wordnet = backend.to_numpy(scores_wordnet)\n",
    "\n",
    "ax = plot_flatmap_from_mapper(scores_wordnet, mapper_file, vmin=0,\n",
    "                              vmax=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the comparison of model performances with a 2D histogram.\n",
    "All ~70k voxels are represented in this histogram, where the diagonal\n",
    "corresponds to identical performance for both models. A distibution deviating\n",
    "from the diagonal means that one model has better predictive performances\n",
    "than the other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from voxelwise_tutorials.viz import plot_hist2d\n",
    "\n",
    "ax = plot_hist2d(scores_wordnet, scores_motion_energy)\n",
    "ax.set(title='Generalization R2 scores', xlabel='semantic wordnet model',\n",
    "       ylabel='motion energy model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the well predicted voxels are different in the two models.\n",
    "To further describe these differences, we can plot both performances on the\n",
    "same flatmap, using a 2D colormap.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from voxelwise_tutorials.viz import plot_2d_flatmap_from_mapper\n",
    "\n",
    "mapper_file = os.path.join(directory, \"mappers\", f\"{subject}_mappers.hdf\")\n",
    "ax = plot_2d_flatmap_from_mapper(scores_wordnet, scores_motion_energy,\n",
    "                                 mapper_file, vmin=0, vmax=0.25, vmin2=0,\n",
    "                                 vmax2=0.5, label_1=\"wordnet\",\n",
    "                                 label_2=\"motion energy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue regions are well predicted by the motion-energy features, the orange\n",
    "regions are well predicted by the wordnet features, and the white regions are\n",
    "well predicted by both feature spaces.\n",
    "\n",
    "Interestingly, a large part of the visual semantic areas are not only well\n",
    "predicted by the wordnet features, but also by the motion-energy features, as\n",
    "indicated by the white color. Since these two features spaces encode quite\n",
    "different information, two interpretations are possible. In the first\n",
    "interpretation, the two feature spaces encode complementary information, and\n",
    "could be used jointly to further increase the generalization performances. In\n",
    "the second interpretation, both feature spaces encode the same information,\n",
    "because of spurious correlation in the stimulus. For example, all faces in\n",
    "the stimulus might be located in the same part of the visual field, thus a\n",
    "motion-energy feature at this location might contain all the necessary\n",
    "information to predict the presence of a face, without specifically encoding\n",
    "for the semantic of faces.\n",
    "\n",
    "To better disentangle the two feature spaces, we developed a joint model\n",
    "called `banded ridge regression` [2]_, which fits multiple feature spaces\n",
    "simultaneously with optimal regularization for each feature space. This model\n",
    "is described in the next example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------\n",
    "\n",
    ".. [1] Nishimoto, S., Vu, A. T., Naselaris, T., Benjamini, Y., Yu,\n",
    "    B., & Gallant, J. L. (2011). Reconstructing visual experiences from brain\n",
    "    activity evoked by natural movies. Current Biology, 21(19), 1641-1646.\n",
    "\n",
    ".. [2] Nunez-Elizalde, A. O., Huth, A. G., & Gallant, J. L. (2019).\n",
    "    Voxelwise encoding models with non-spherical multivariate normal priors.\n",
    "    Neuroimage, 197, 482-492.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 05_plot_banded_ridge_model.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Fit a banded ridge model with both wordnet and motion energy features\n",
    "\n",
    "\n",
    "In this example, we model the fMRI responses with a `banded ridge regression`,\n",
    "with two different feature spaces: motion energy, and wordnet categories.\n",
    "\n",
    "*Banded ridge regression:* Since the relative scaling of both feature spaces is\n",
    "unknown, we use two regularization hyperparameters (one per feature space) in a\n",
    "model called banded ridge regression [1]_. Just like with ridge regression, we\n",
    "optimize the hyperparameters over cross-validation. An efficient implementation\n",
    "of this model is available in the `himalaya\n",
    "<https://github.com/gallantlab/himalaya>`_ package.\n",
    "\n",
    "*Running time:* This example is more computationally intensive than previous\n",
    "examples. With a GPU backend, the fitting of this model takes around 6 minutes.\n",
    "With a CPU backend, it can last 10 times more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path of the data directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from voxelwise_tutorials.io import get_data_home\n",
    "directory = os.path.join(get_data_home(), \"vim-5\")\n",
    "print(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modify to use another subject\n",
    "subject = \"S01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data\n",
    "-------------\n",
    "\n",
    "As in the previous examples, we first load the fMRI responses, which are our\n",
    "regression targets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from voxelwise_tutorials.io import load_hdf5_array\n",
    "\n",
    "file_name = os.path.join(directory, \"responses\", f\"{subject}_responses.hdf\")\n",
    "Y_train = load_hdf5_array(file_name, key=\"Y_train\")\n",
    "Y_test = load_hdf5_array(file_name, key=\"Y_test\")\n",
    "\n",
    "print(\"(n_samples_train, n_voxels) =\", Y_train.shape)\n",
    "print(\"(n_repeats, n_samples_test, n_voxels) =\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We average the test repeats, to remove the non-repeatable part of fMRI\n",
    "responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_test = Y_test.mean(0)\n",
    "\n",
    "print(\"(n_samples_test, n_voxels) =\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fill potential NaN (not-a-number) values with zeros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train = np.nan_to_num(Y_train)\n",
    "Y_test = np.nan_to_num(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we make sure the targets are centered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train -= Y_train.mean(0)\n",
    "Y_test -= Y_test.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load both feature spaces, that are going to be used for the\n",
    "linear regression model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = [\"wordnet\", \"motion_energy\"]\n",
    "\n",
    "Xs_train = []\n",
    "Xs_test = []\n",
    "n_features_list = []\n",
    "for feature_space in feature_names:\n",
    "    file_name = os.path.join(directory, \"features\", f\"{feature_space}.hdf\")\n",
    "    Xi_train = load_hdf5_array(file_name, key=\"X_train\")\n",
    "    Xi_test = load_hdf5_array(file_name, key=\"X_test\")\n",
    "\n",
    "    Xs_train.append(Xi_train.astype(dtype=\"float32\"))\n",
    "    Xs_test.append(Xi_test.astype(dtype=\"float32\"))\n",
    "    n_features_list.append(Xi_train.shape[1])\n",
    "\n",
    "# concatenate the feature spaces\n",
    "X_train = np.concatenate(Xs_train, 1)\n",
    "X_test = np.concatenate(Xs_test, 1)\n",
    "\n",
    "print(\"(n_samples_train, n_features_total) =\", X_train.shape)\n",
    "print(\"(n_samples_test, n_features_total) =\", X_test.shape)\n",
    "print(\"[n_features_wordnet, n_features_motion_energy] =\", n_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the cross-validation scheme\n",
    "----------------------------------\n",
    "\n",
    "We define again a leave-one-run-out cross-validation split scheme.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import check_cv\n",
    "from voxelwise_tutorials.utils import generate_leave_one_run_out\n",
    "\n",
    "# indice of first sample of each run\n",
    "run_onsets = load_hdf5_array(file_name, key=\"run_onsets\")\n",
    "print(run_onsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a cross-validation splitter, compatible with ``scikit-learn`` API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples_train = X_train.shape[0]\n",
    "cv = generate_leave_one_run_out(n_samples_train, run_onsets)\n",
    "cv = check_cv(cv)  # copy the cross-validation splitter into a reusable list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model\n",
    "----------------\n",
    "\n",
    "The model pipeline contains similar steps than the pipeline from previous\n",
    "examples. We remove the mean of each feature with a ``StandardScaler``,\n",
    "and add delays with a ``Delayer``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from voxelwise_tutorials.delayer import Delayer\n",
    "from himalaya.backend import set_backend\n",
    "backend = set_backend(\"torch_cuda\", on_error=\"warn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the banded ridge model, we use ``himalaya``'s\n",
    "``MultipleKernelRidgeCV`` model, with a separate linear kernel per feature\n",
    "space. Similarly to ``KernelRidgeCV``, the model optimizes the\n",
    "hyperparameters over cross-validation. However, while ``KernelRidgeCV`` has\n",
    "to optimize only one hyperparameter (``alpha``), ``MultipleKernelRidgeCV``\n",
    "has to optimize ``m`` hyperparameters, where ``m`` is the number of feature\n",
    "spaces (here ``m = 2``). To do so, the model implements two different\n",
    "solvers, one using hyperparameter random search, and one using hyperparameter\n",
    "gradient descent. For large number of targets, we recommend using the\n",
    "random-search solver.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class takes a number of common parameters during initialization, such as\n",
    "``kernels``, or ``solver``. Since the solver parameters are rather different\n",
    "depending on the solver, they are passed in a ``solver_params`` dictionary\n",
    "parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from himalaya.kernel_ridge import MultipleKernelRidgeCV\n",
    "\n",
    "# Here we will use the \"random_search\" solver.\n",
    "solver = \"random_search\"\n",
    "\n",
    "# We can check its specific parameters in the function docstring:\n",
    "solver_function = MultipleKernelRidgeCV.ALL_SOLVERS[solver]\n",
    "print(\"Docstring of the function %s:\" % solver_function.__name__)\n",
    "print(solver_function.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter random-search solver separates the hyperparameters into a\n",
    "shared regularization ``alpha`` and a vector of positive kernel weights which\n",
    "sum to one. This separation of hyperparameters allows to explore efficiently\n",
    "a large grid of values for ``alpha`` for each sampled kernel weights vector.\n",
    "\n",
    "We use *20* random-search iterations to have a reasonably fast example. To\n",
    "have better results, especially for larger number of feature spaces, one\n",
    "might need more iterations. (Note that there is currently no stopping\n",
    "criterion in the random-search method.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_iter = 20\n",
    "\n",
    "alphas = np.logspace(1, 20, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch parameters, used to reduce the necessary GPU memory. A larger value\n",
    "will be a bit faster, but the solver might crash if it is out of memory.\n",
    "Optimal values depend on the size of your dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_targets_batch = 200\n",
    "n_alphas_batch = 5\n",
    "n_targets_batch_refit = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put all these parameters in a dictionary ``solver_params``, and define\n",
    "the main estimator ``MultipleKernelRidgeCV``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solver_params = dict(n_iter=n_iter, alphas=alphas,\n",
    "                     n_targets_batch=n_targets_batch,\n",
    "                     n_alphas_batch=n_alphas_batch,\n",
    "                     n_targets_batch_refit=n_targets_batch_refit)\n",
    "\n",
    "mkr_model = MultipleKernelRidgeCV(kernels=\"precomputed\", solver=solver,\n",
    "                                  solver_params=solver_params, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a bit more work than in previous examples before defining the full\n",
    "pipeline, since the banded ridge model requires `multiple` precomputed\n",
    "kernels, one for each feature space. To compute them, we use the\n",
    "``ColumnKernelizer``, which can create multiple kernels from different\n",
    "column of your features array. ``ColumnKernelizer`` works similarly to\n",
    "``scikit-learn``'s ``ColumnTransformer``, but instead of returning a\n",
    "concatenation of transformed features, it returns a stack of kernels,\n",
    "as required in ``MultipleKernelRidgeCV(kernels=\"precomputed\")``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a different ``Kernelizer`` for each feature space.\n",
    "Here we use a linear kernel for all feature spaces, but ``ColumnKernelizer``\n",
    "accepts any ``Kernelizer``, or ``scikit-learn`` ``Pipeline`` ending with a\n",
    "``Kernelizer``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from himalaya.kernel_ridge import Kernelizer\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')  # requires scikit-learn 0.23\n",
    "\n",
    "preprocess_pipeline = make_pipeline(\n",
    "    StandardScaler(with_mean=True, with_std=False),\n",
    "    Delayer(delays=[1, 2, 3, 4]),\n",
    "    Kernelizer(kernel=\"linear\"),\n",
    ")\n",
    "preprocess_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column kernelizer applies a different pipeline on each selection of\n",
    "features, here defined with ``slices``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from himalaya.kernel_ridge import ColumnKernelizer\n",
    "\n",
    "# Find the start and end of each feature space in the concatenated ``X_train``.\n",
    "start_and_end = np.concatenate([[0], np.cumsum(n_features_list)])\n",
    "slices = [\n",
    "    slice(start, end)\n",
    "    for start, end in zip(start_and_end[:-1], start_and_end[1:])\n",
    "]\n",
    "slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kernelizers_tuples = [(name, preprocess_pipeline, slice_)\n",
    "                      for name, slice_ in zip(feature_names, slices)]\n",
    "column_kernelizer = ColumnKernelizer(kernelizers_tuples)\n",
    "column_kernelizer\n",
    "\n",
    "# (Note that ``ColumnKernelizer`` has a parameter ``n_jobs`` to parallelize\n",
    "# each ``Kernelizer``, yet such parallelism does not work with GPU arrays.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can define the model pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    column_kernelizer,\n",
    "    mkr_model,\n",
    ")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model\n",
    "-------------\n",
    "\n",
    "We fit on the train set, and score on the test set.\n",
    "\n",
    "With a GPU backend, the fitting of this model takes around 6 minutes. With a\n",
    "CPU backend, it can last 10 times more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, Y_train)\n",
    "\n",
    "scores = pipeline.score(X_test, Y_test)\n",
    "scores = backend.to_numpy(scores)\n",
    "\n",
    "print(\"(n_voxels,) =\", scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with a ridge model\n",
    "--------------------------\n",
    "\n",
    "We can compare with a baseline model, which does not use one hyperparameter\n",
    "per feature space, but instead shares the same hyperparameter for all feature\n",
    "spaces.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from himalaya.kernel_ridge import KernelRidgeCV\n",
    "\n",
    "pipeline_baseline = make_pipeline(\n",
    "    StandardScaler(with_mean=True, with_std=False),\n",
    "    Delayer(delays=[1, 2, 3, 4]),\n",
    "    KernelRidgeCV(\n",
    "        alphas=alphas, cv=cv,\n",
    "        solver_params=dict(n_targets_batch=n_targets_batch,\n",
    "                           n_alphas_batch=n_alphas_batch,\n",
    "                           n_targets_batch_refit=n_targets_batch_refit)),\n",
    ")\n",
    "pipeline_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline_baseline.fit(X_train, Y_train)\n",
    "scores_baseline = pipeline_baseline.score(X_test, Y_test)\n",
    "scores_baseline = backend.to_numpy(scores_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the comparison of model performances with a 2D histogram.\n",
    "All 70k voxels are represented in this histogram, where the diagonal\n",
    "corresponds to identical performance for both models. A distibution deviating\n",
    "from the diagonal means that one model has better predictive performances\n",
    "than the other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from voxelwise_tutorials.viz import plot_hist2d\n",
    "\n",
    "ax = plot_hist2d(scores_baseline, scores)\n",
    "ax.set(title='Generalization R2 scores', xlabel='KernelRidgeCV',\n",
    "       ylabel='MultipleKernelRidgeCV')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the banded ridge model (``MultipleKernelRidgeCV``) outperforms\n",
    "the ridge model (``KernelRidegeCV``). Indeed, banded ridge regression is able\n",
    "to find the optimal scalings of each feature space, independently on each\n",
    "voxel. Banded ridge regression is thus able to perform a soft selection\n",
    "between the available feature spaces, based on the cross-validation\n",
    "performances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the banded ridge split\n",
    "---------------------------\n",
    "\n",
    "On top of better performances, banded ridge regression also gives a way to\n",
    "disentangle the two feature spaces. To do so, we take the kernel weights and\n",
    "the ridge (dual) weights corresponding to each feature space, and use them to\n",
    "split the prediction on each feature space.\n",
    "\n",
    "\\begin{align}\\hat{y} = \\sum_i^m \\hat{y}_i = \\sum_i^m \\gamma_i K_i \\hat{w}\\end{align}\n",
    "\n",
    "Then, we use these split predictions to compute split math:`\\tilde{R}^2_i`\n",
    "scores, corrected so that there sum is equal to the math:`R^2` score of the\n",
    "full prediction math:`\\hat{y}`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from himalaya.scoring import r2_score_split\n",
    "\n",
    "Y_test_pred_split = pipeline.predict(X_test, split=True)\n",
    "split_scores = r2_score_split(Y_test, Y_test_pred_split)\n",
    "split_scores.shape\n",
    "\n",
    "print(\"(n_kernels, n_samples_test, n_voxels) =\", Y_test_pred_split.shape)\n",
    "print(\"(n_kernels, n_voxels) =\", split_scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot the split scores on a flatmap with a 2D colormap.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from voxelwise_tutorials.viz import plot_2d_flatmap_from_mapper\n",
    "\n",
    "mapper_file = os.path.join(directory, \"mappers\", f\"{subject}_mappers.hdf\")\n",
    "ax = plot_2d_flatmap_from_mapper(split_scores[0], split_scores[1],\n",
    "                                 mapper_file, vmin=0, vmax=0.25, vmin2=0,\n",
    "                                 vmax2=0.5, label_1=feature_names[0],\n",
    "                                 label_2=feature_names[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue regions are predicted using the motion-energy features, the orange\n",
    "regions are predicted using the wordnet features, and the white regions are\n",
    "well predicted using both feature spaces.\n",
    "\n",
    "Compared to the last figure of the previous example, we see that most white\n",
    "regions have been replaced by either blue or orange regions. Indeed, the\n",
    "banded ridge regression disentangled the two feature spaces in voxels where\n",
    "both feature spaces had good performances (see previous example). The\n",
    "disentanglement is consistent with prior knowledge. For example, the\n",
    "motion-energy features are selected in the early visual cortex, while the\n",
    "wordnet features are selected in the semantic visual areas. For more\n",
    "discussions about these results, we refer the reader to the original\n",
    "publication [1]_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------\n",
    "\n",
    ".. [1] Nunez-Elizalde, A. O., Huth, A. G., & Gallant, J. L. (2019).\n",
    "    Voxelwise encoding models with non-spherical multivariate normal priors.\n",
    "    Neuroimage, 197, 482-492.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del pipeline, pipeline_baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
