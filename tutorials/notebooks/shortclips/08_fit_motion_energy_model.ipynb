{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Fit a ridge model with motion-energy features\n",
        "\n",
        "In this example, we model the fMRI responses with motion-energy features\n",
        "extracted from the movie stimulus. The model is a regularized linear regression\n",
        "model.\n",
        "\n",
        "This tutorial reproduces part of the analysis described in {cite:t}`nishimoto2011`. See the original publication for more details about the experiment, the\n",
        "motion-energy features, along with more results and more discussions.\n",
        "\n",
        "*Motion-energy features:* Motion-energy features result from filtering a video\n",
        "stimulus with spatio-temporal Gabor filters. A pyramid of filters is used to\n",
        "compute the motion-energy features at multiple spatial and temporal scales.\n",
        "Motion-energy features were introduced in {cite:t}`nishimoto2011`. The downloaded \n",
        "dataset contains the pre-computed motion-energy features for the movie stimulus used \n",
        "in the experiment. You can see how to extract these motion-energy features in the\n",
        "[Extract motion-energy features](07_extract_motion_energy.ipynb) tutorial.\n",
        "\n",
        "*Summary:* As in the previous example, we first concatenate the features with\n",
        "multiple delays, to account for the slow hemodynamic response. A linear\n",
        "regression model then weights each delayed feature with a different weight, to\n",
        "build a predictive model of BOLD activity. Again, the linear regression is\n",
        "regularized to improve robustness to correlated features and to improve\n",
        "generalization. The optimal regularization hyperparameter is selected\n",
        "independently on each voxel over a grid-search with cross-validation. Finally,\n",
        "the model generalization performance is evaluated on a held-out test set,\n",
        "comparing the model predictions with the ground-truth fMRI responses.\n",
        "\n",
        ":::{note}\n",
        "It should take less than 5 minutes to run the model fitting in this tutorial on a GPU. If you are using a CPU, it may take longer.\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Path of the data directory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.io import get_data_home\n",
        "directory = get_data_home(dataset=\"shortclips\")\n",
        "print(directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# modify to use another subject\n",
        "subject = \"S01\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the data\n",
        "\n",
        "We first load the fMRI responses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from voxelwise_tutorials.io import load_hdf5_array\n",
        "\n",
        "file_name = os.path.join(directory, \"responses\", f\"{subject}_responses.hdf\")\n",
        "Y_train = load_hdf5_array(file_name, key=\"Y_train\")\n",
        "Y_test = load_hdf5_array(file_name, key=\"Y_test\")\n",
        "\n",
        "print(\"(n_samples_train, n_voxels) =\", Y_train.shape)\n",
        "print(\"(n_repeats, n_samples_test, n_voxels) =\", Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We average the test repeats, to remove the non-repeatable part of fMRI\n",
        "responses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Y_test = Y_test.mean(0)\n",
        "\n",
        "print(\"(n_samples_test, n_voxels) =\", Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We fill potential NaN (not-a-number) values with zeros.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Y_train = np.nan_to_num(Y_train)\n",
        "Y_test = np.nan_to_num(Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we load the precomputed \"motion-energy\" features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feature_space = \"motion_energy\"\n",
        "file_name = os.path.join(directory, \"features\", f\"{feature_space}.hdf\")\n",
        "X_train = load_hdf5_array(file_name, key=\"X_train\")\n",
        "X_test = load_hdf5_array(file_name, key=\"X_test\")\n",
        "\n",
        "print(\"(n_samples_train, n_features) =\", X_train.shape)\n",
        "print(\"(n_samples_test, n_features) =\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the cross-validation scheme\n",
        "\n",
        "We define the same leave-one-run-out cross-validation split as in the\n",
        "previous example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import check_cv\n",
        "from voxelwise_tutorials.utils import generate_leave_one_run_out\n",
        "\n",
        "# indice of first sample of each run\n",
        "run_onsets = load_hdf5_array(file_name, key=\"run_onsets\")\n",
        "print(run_onsets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a cross-validation splitter, compatible with ``scikit-learn`` API.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples_train = X_train.shape[0]\n",
        "cv = generate_leave_one_run_out(n_samples_train, run_onsets)\n",
        "cv = check_cv(cv)  # copy the cross-validation splitter into a reusable list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the model\n",
        "\n",
        "We define the same model as in the previous example. See the previous\n",
        "example for more details about the model definition.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from voxelwise_tutorials.delayer import Delayer\n",
        "from himalaya.kernel_ridge import KernelRidgeCV\n",
        "from himalaya.backend import set_backend\n",
        "backend = set_backend(\"torch_cuda\", on_error=\"warn\")\n",
        "\n",
        "X_train = X_train.astype(\"float32\")\n",
        "X_test = X_test.astype(\"float32\")\n",
        "\n",
        "alphas = np.logspace(1, 20, 20)\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    StandardScaler(with_mean=True, with_std=False),\n",
        "    Delayer(delays=[1, 2, 3, 4]),\n",
        "    KernelRidgeCV(\n",
        "        alphas=alphas, cv=cv,\n",
        "        solver_params=dict(n_targets_batch=500, n_alphas_batch=5,\n",
        "                           n_targets_batch_refit=100)),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn import set_config\n",
        "set_config(display='diagram')  # requires scikit-learn 0.23\n",
        "pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit the model\n",
        "\n",
        "We fit on the train set, and score on the test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline.fit(X_train, Y_train)\n",
        "\n",
        "scores_motion_energy = pipeline.score(X_test, Y_test)\n",
        "scores_motion_energy = backend.to_numpy(scores_motion_energy)\n",
        "\n",
        "print(\"(n_voxels,) =\", scores_motion_energy.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the model performances\n",
        "The performances are computed using the $R^2$ scores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from voxelwise_tutorials.viz import plot_flatmap_from_mapper\n",
        "\n",
        "mapper_file = os.path.join(directory, \"mappers\", f\"{subject}_mappers.hdf\")\n",
        "ax = plot_flatmap_from_mapper(scores_motion_energy, mapper_file, vmin=0,\n",
        "                              vmax=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The motion-energy features lead to large generalization scores in the\n",
        "early visual cortex (V1, V2, V3, ...). For more discussions about these\n",
        "results, we refer the reader to the original publication {cite}`nishimoto2011`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare with the wordnet model\n",
        "\n",
        "Interestingly, the motion-energy model performs well in different brain\n",
        "regions than the semantic \"wordnet\" model fitted in the previous example. To\n",
        "compare the two models, we first need to fit again the wordnet model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feature_space = \"wordnet\"\n",
        "file_name = os.path.join(directory, \"features\", f\"{feature_space}.hdf\")\n",
        "X_train = load_hdf5_array(file_name, key=\"X_train\")\n",
        "X_test = load_hdf5_array(file_name, key=\"X_test\")\n",
        "\n",
        "X_train = X_train.astype(\"float32\")\n",
        "X_test = X_test.astype(\"float32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can create an unfitted copy of the pipeline with the ``clone`` function,\n",
        "or simply call fit again if we do not need to reuse the previous model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from sklearn.base import clone\n",
        "    pipeline_wordnet = clone(pipeline)\n",
        "    pipeline_wordnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline.fit(X_train, Y_train)\n",
        "scores_wordnet = pipeline.score(X_test, Y_test)\n",
        "scores_wordnet = backend.to_numpy(scores_wordnet)\n",
        "\n",
        "ax = plot_flatmap_from_mapper(scores_wordnet, mapper_file, vmin=0,\n",
        "                              vmax=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also plot the comparison of model prediction accuracies with a 2D\n",
        "histogram. All ~70k voxels are represented in this histogram, where the\n",
        "diagonal corresponds to identical prediction accuracy for both models. A\n",
        "distribution deviating from the diagonal means that one model has better\n",
        "predictive performance than the other.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.viz import plot_hist2d\n",
        "\n",
        "ax = plot_hist2d(scores_wordnet, scores_motion_energy)\n",
        "ax.set(title='Generalization R2 scores', xlabel='semantic wordnet model',\n",
        "       ylabel='motion energy model')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interestingly, the well predicted voxels are different in the two models.\n",
        "To further describe these differences, we can plot both performances on the\n",
        "same flatmap, using a 2D colormap.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.viz import plot_2d_flatmap_from_mapper\n",
        "\n",
        "mapper_file = os.path.join(directory, \"mappers\", f\"{subject}_mappers.hdf\")\n",
        "ax = plot_2d_flatmap_from_mapper(scores_wordnet, scores_motion_energy,\n",
        "                                 mapper_file, vmin=0, vmax=0.25, vmin2=0,\n",
        "                                 vmax2=0.5, label_1=\"wordnet\",\n",
        "                                 label_2=\"motion energy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The blue regions are well predicted by the motion-energy features, the orange\n",
        "regions are well predicted by the wordnet features, and the white regions are\n",
        "well predicted by both feature spaces.\n",
        "\n",
        "A large part of the visual semantic areas are not only well predicted by the\n",
        "wordnet features, but also by the motion-energy features, as indicated by the\n",
        "white color. Since these two features spaces encode quite different\n",
        "information, two interpretations are possible. In the first interpretation,\n",
        "the two feature spaces encode complementary information, and could be used\n",
        "jointly to further increase the generalization performance. In the second\n",
        "interpretation, both feature spaces encode the same information, because of\n",
        "spurious stimulus correlations. For example, imagine that the visual stimulus\n",
        "contained faces that appeared consistetly in the same portion of the visual\n",
        "field. In this case, position in the visual field would be perfectly\n",
        "correlated with the \"face\" semantic category. Thus, motion-energy features\n",
        "could predict responses in face-responsive areas without encoding any\n",
        "semantic information.\n",
        "\n",
        "To better disentangle the two feature spaces, we developed a joint model\n",
        "called `banded ridge regression` {cite}`nunez2019,dupre2022`, which fits multiple feature spaces\n",
        "simultaneously with optimal regularization for each feature space. This model\n",
        "is described in the next example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "```{bibliography}\n",
        ":filter: docname in docnames\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
