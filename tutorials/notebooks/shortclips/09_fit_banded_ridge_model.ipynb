{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fit a voxelwise encoding model with both WordNet and motion-energy features\n",
        "\n",
        "In this example, we model the fMRI responses with a *banded ridge regression* \n",
        "with two different feature spaces: motion energy and wordnet categories.\n",
        "\n",
        "*Banded ridge regression:* Since the relative scaling of both feature spaces is\n",
        "unknown, we use two regularization hyperparameters (one per feature space) in a\n",
        "model called banded ridge regression {cite}`nunez2019,dupre2022`. \n",
        "Just like with ridge regression, we optimize the hyperparameters over cross-validation. \n",
        "An efficient implementation of this model is available in the \n",
        "[himalaya](https://github.com/gallantlab/himalaya) package.\n",
        "\n",
        ":::{admonition} Long running time on a CPU!\n",
        ":class: warning\n",
        "This example is more computationally intensive than the previous examples. \n",
        "With a GPU backend, model fitting takes around 6 minutes.\n",
        "With a CPU backend, it can take more than an hour.\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Path of the data directory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.io import get_data_home\n",
        "directory = get_data_home(dataset=\"shortclips\")\n",
        "print(directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# modify to use another subject\n",
        "subject = \"S01\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the data\n",
        "\n",
        "As in the previous examples, we first load the fMRI responses, which are our\n",
        "regression targets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from voxelwise_tutorials.io import load_hdf5_array\n",
        "\n",
        "file_name = os.path.join(directory, \"responses\", f\"{subject}_responses.hdf\")\n",
        "Y_train = load_hdf5_array(file_name, key=\"Y_train\")\n",
        "Y_test = load_hdf5_array(file_name, key=\"Y_test\")\n",
        "\n",
        "print(\"(n_samples_train, n_voxels) =\", Y_train.shape)\n",
        "print(\"(n_repeats, n_samples_test, n_voxels) =\", Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also compute the explainable variance, to exclude voxels with low\n",
        "explainable variance from the fit, and speed up the model fitting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.utils import explainable_variance\n",
        "ev = explainable_variance(Y_test)\n",
        "print(\"(n_voxels,) =\", ev.shape)\n",
        "\n",
        "mask = ev > 0.1\n",
        "print(\"(n_voxels_mask,) =\", ev[mask].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We average the test repeats, to remove the non-repeatable part of fMRI\n",
        "responses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Y_test = Y_test.mean(0)\n",
        "\n",
        "print(\"(n_samples_test, n_voxels) =\", Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We fill potential NaN (not-a-number) values with zeros.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Y_train = np.nan_to_num(Y_train)\n",
        "Y_test = np.nan_to_num(Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we make sure the targets are centered.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Y_train -= Y_train.mean(0)\n",
        "Y_test -= Y_test.mean(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we load both feature spaces, that are going to be used for the\n",
        "linear regression model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feature_names = [\"wordnet\", \"motion_energy\"]\n",
        "\n",
        "Xs_train = []\n",
        "Xs_test = []\n",
        "n_features_list = []\n",
        "for feature_space in feature_names:\n",
        "    file_name = os.path.join(directory, \"features\", f\"{feature_space}.hdf\")\n",
        "    Xi_train = load_hdf5_array(file_name, key=\"X_train\")\n",
        "    Xi_test = load_hdf5_array(file_name, key=\"X_test\")\n",
        "\n",
        "    Xs_train.append(Xi_train.astype(dtype=\"float32\"))\n",
        "    Xs_test.append(Xi_test.astype(dtype=\"float32\"))\n",
        "    n_features_list.append(Xi_train.shape[1])\n",
        "\n",
        "# concatenate the feature spaces\n",
        "X_train = np.concatenate(Xs_train, 1)\n",
        "X_test = np.concatenate(Xs_test, 1)\n",
        "\n",
        "print(\"(n_samples_train, n_features_total) =\", X_train.shape)\n",
        "print(\"(n_samples_test, n_features_total) =\", X_test.shape)\n",
        "print(\"[n_features_wordnet, n_features_motion_energy] =\", n_features_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the cross-validation scheme\n",
        "\n",
        "We define again a leave-one-run-out cross-validation split scheme.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import check_cv\n",
        "from voxelwise_tutorials.utils import generate_leave_one_run_out\n",
        "\n",
        "# indice of first sample of each run\n",
        "run_onsets = load_hdf5_array(file_name, key=\"run_onsets\")\n",
        "print(run_onsets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a cross-validation splitter, compatible with ``scikit-learn`` API.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples_train = X_train.shape[0]\n",
        "cv = generate_leave_one_run_out(n_samples_train, run_onsets)\n",
        "cv = check_cv(cv)  # copy the cross-validation splitter into a reusable list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the model\n",
        "\n",
        "The model pipeline contains similar steps than the pipeline from previous\n",
        "examples. We remove the mean of each feature with a ``StandardScaler``,\n",
        "and add delays with a ``Delayer``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from voxelwise_tutorials.delayer import Delayer\n",
        "from himalaya.backend import set_backend\n",
        "backend = set_backend(\"torch_cuda\", on_error=\"warn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To fit the banded ridge model, we use ``himalaya``'s\n",
        "``MultipleKernelRidgeCV`` model, with a separate linear kernel per feature\n",
        "space. Similarly to ``KernelRidgeCV``, the model optimizes the\n",
        "hyperparameters over cross-validation. However, while ``KernelRidgeCV`` has\n",
        "to optimize only one hyperparameter (``alpha``), ``MultipleKernelRidgeCV``\n",
        "has to optimize ``m`` hyperparameters, where ``m`` is the number of feature\n",
        "spaces (here ``m = 2``). To do so, the model implements two different\n",
        "solvers, one using hyperparameter random search, and one using hyperparameter\n",
        "gradient descent. For large number of targets, we recommend using the\n",
        "random-search solver.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The class takes a number of common parameters during initialization, such as\n",
        "``kernels``, or ``solver``. Since the solver parameters vary depending on the\n",
        "solver used, they are passed as a ``solver_params`` dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from himalaya.kernel_ridge import MultipleKernelRidgeCV\n",
        "\n",
        "# Here we will use the \"random_search\" solver.\n",
        "solver = \"random_search\"\n",
        "\n",
        "# We can check its specific parameters in the function docstring:\n",
        "solver_function = MultipleKernelRidgeCV.ALL_SOLVERS[solver]\n",
        "print(\"Docstring of the function %s:\" % solver_function.__name__)\n",
        "print(solver_function.__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The hyperparameter random-search solver separates the hyperparameters into a\n",
        "shared regularization ``alpha`` and a vector of positive kernel weights which\n",
        "sum to one. This separation of hyperparameters allows to explore efficiently\n",
        "a large grid of values for ``alpha`` for each sampled kernel weights vector.\n",
        "\n",
        "We use `n_iter=20` random-search iterations to have a reasonably fast example. To\n",
        "have better results, especially for larger number of feature spaces, one\n",
        "might need more iterations. (Note that there is currently no stopping\n",
        "criterion in the random-search method.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_iter = 20\n",
        "\n",
        "alphas = np.logspace(1, 20, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Batch parameters, used to reduce the necessary GPU memory. A larger value\n",
        "will be a bit faster, but the solver might crash if it is out of memory.\n",
        "Optimal values depend on the size of your dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_targets_batch = 200\n",
        "n_alphas_batch = 5\n",
        "n_targets_batch_refit = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We put all these parameters in a dictionary ``solver_params``, and define\n",
        "the main estimator ``MultipleKernelRidgeCV``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "solver_params = dict(n_iter=n_iter, alphas=alphas,\n",
        "                     n_targets_batch=n_targets_batch,\n",
        "                     n_alphas_batch=n_alphas_batch,\n",
        "                     n_targets_batch_refit=n_targets_batch_refit)\n",
        "\n",
        "mkr_model = MultipleKernelRidgeCV(kernels=\"precomputed\", solver=solver,\n",
        "                                  solver_params=solver_params, cv=cv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need a bit more work than in previous examples before defining the full\n",
        "pipeline, since the banded ridge model requires `multiple` precomputed\n",
        "kernels, one for each feature space. To compute them, we use the\n",
        "``ColumnKernelizer``, which can create multiple kernels from different\n",
        "column of your features array. ``ColumnKernelizer`` works similarly to\n",
        "``scikit-learn``'s ``ColumnTransformer``, but instead of returning a\n",
        "concatenation of transformed features, it returns a stack of kernels,\n",
        "as required in ``MultipleKernelRidgeCV(kernels=\"precomputed\")``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we create a different ``Kernelizer`` for each feature space.\n",
        "Here we use a linear kernel for all feature spaces, but ``ColumnKernelizer``\n",
        "accepts any ``Kernelizer``, or ``scikit-learn`` ``Pipeline`` ending with a\n",
        "``Kernelizer``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from himalaya.kernel_ridge import Kernelizer\n",
        "from sklearn import set_config\n",
        "set_config(display='diagram')  # requires scikit-learn 0.23\n",
        "\n",
        "preprocess_pipeline = make_pipeline(\n",
        "    StandardScaler(with_mean=True, with_std=False),\n",
        "    Delayer(delays=[1, 2, 3, 4]),\n",
        "    Kernelizer(kernel=\"linear\"),\n",
        ")\n",
        "preprocess_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The column kernelizer applies a different pipeline on each selection of\n",
        "features, here defined with ``slices``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from himalaya.kernel_ridge import ColumnKernelizer\n",
        "\n",
        "# Find the start and end of each feature space in the concatenated ``X_train``.\n",
        "start_and_end = np.concatenate([[0], np.cumsum(n_features_list)])\n",
        "slices = [\n",
        "    slice(start, end)\n",
        "    for start, end in zip(start_and_end[:-1], start_and_end[1:])\n",
        "]\n",
        "slices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kernelizers_tuples = [(name, preprocess_pipeline, slice_)\n",
        "                      for name, slice_ in zip(feature_names, slices)]\n",
        "column_kernelizer = ColumnKernelizer(kernelizers_tuples)\n",
        "column_kernelizer\n",
        "\n",
        "# (Note that ``ColumnKernelizer`` has a parameter ``n_jobs`` to parallelize\n",
        "# each ``Kernelizer``, yet such parallelism does not work with GPU arrays.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we can define the model pipeline.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline = make_pipeline(\n",
        "    column_kernelizer,\n",
        "    mkr_model,\n",
        ")\n",
        "pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit the model\n",
        "\n",
        "We fit on the train set, and score on the test set.\n",
        "\n",
        "To speed up the fit and to limit the memory peaks, we only fit on\n",
        "voxels with explainable variance above 0.1. If your GPU has sufficient memory, you can\n",
        "avoid masking the data and fit the model on all voxels. Note also that this masking is\n",
        "performed here only for the purposes of the tutorial, and it should not be performed\n",
        "for an actual analysis.\n",
        "\n",
        "With a GPU backend, the fitting of this model takes around 6 minutes. With a\n",
        "CPU backend, it can last 10 times more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline.fit(X_train, Y_train[:, mask])\n",
        "\n",
        "scores_mask = pipeline.score(X_test, Y_test[:, mask])\n",
        "scores_mask = backend.to_numpy(scores_mask)\n",
        "print(\"(n_voxels_mask,) =\", scores_mask.shape)\n",
        "\n",
        "# Then we extend the scores to all voxels, giving a score of zero to unfitted\n",
        "# voxels.\n",
        "n_voxels = Y_train.shape[1]\n",
        "scores = np.zeros(n_voxels)\n",
        "scores[mask] = scores_mask\n",
        "print(\"(n_voxels,) =\", scores.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare with a ridge model\n",
        "\n",
        "We can compare with a baseline model, which does not use one hyperparameter\n",
        "per feature space, but instead shares the same hyperparameter for all feature\n",
        "spaces.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from himalaya.kernel_ridge import KernelRidgeCV\n",
        "\n",
        "pipeline_baseline = make_pipeline(\n",
        "    StandardScaler(with_mean=True, with_std=False),\n",
        "    Delayer(delays=[1, 2, 3, 4]),\n",
        "    KernelRidgeCV(\n",
        "        alphas=alphas, cv=cv,\n",
        "        solver_params=dict(n_targets_batch=n_targets_batch,\n",
        "                           n_alphas_batch=n_alphas_batch,\n",
        "                           n_targets_batch_refit=n_targets_batch_refit)),\n",
        ")\n",
        "pipeline_baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline_baseline.fit(X_train, Y_train[:, mask])\n",
        "scores_baseline_mask = pipeline_baseline.score(X_test, Y_test[:, mask])\n",
        "scores_baseline_mask = backend.to_numpy(scores_baseline_mask)\n",
        "\n",
        "# extend to unfitted voxels\n",
        "n_voxels = Y_train.shape[1]\n",
        "scores_baseline = np.zeros(n_voxels)\n",
        "scores_baseline[mask] = scores_baseline_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we plot the comparison of model prediction accuracies with a 2D\n",
        "histogram. All 70k voxels are represented in this histogram, where the\n",
        "diagonal corresponds to identical model prediction accuracy for both models.\n",
        "A distribution deviating from the diagonal means that one model has better\n",
        "predictive performance than the other.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from voxelwise_tutorials.viz import plot_hist2d\n",
        "\n",
        "ax = plot_hist2d(scores_baseline, scores)\n",
        "ax.set(title='Generalization R2 scores', xlabel='KernelRidgeCV',\n",
        "       ylabel='MultipleKernelRidgeCV')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the banded ridge model (``MultipleKernelRidgeCV``) outperforms\n",
        "the ridge model (``KernelRidegeCV``). Indeed, banded ridge regression is able\n",
        "to find the optimal scalings of each feature space, independently on each\n",
        "voxel. Banded ridge regression is thus able to perform a soft selection\n",
        "between the available feature spaces, based on the cross-validation\n",
        "performances.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the banded ridge split\n",
        "\n",
        "On top of better prediction accuracy, banded ridge regression also gives a\n",
        "way to disentangle the contribution of the two feature spaces. To do so, we\n",
        "take the kernel weights and the ridge (dual) weights corresponding to each\n",
        "feature space, and use them to compute the prediction from each feature space\n",
        "separately.\n",
        "\n",
        "\\begin{align}\\hat{y} = \\sum_i^m \\hat{y}_i = \\sum_i^m \\gamma_i K_i \\hat{w}\\end{align}\n",
        "\n",
        "Then, we use these split predictions to compute split $\\tilde{R}^2_i$\n",
        "scores. These scores are corrected so that their sum is equal to the\n",
        "$R^2$ score of the full prediction $\\hat{y}$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from himalaya.scoring import r2_score_split\n",
        "\n",
        "Y_test_pred_split = pipeline.predict(X_test, split=True)\n",
        "split_scores_mask = r2_score_split(Y_test[:, mask], Y_test_pred_split)\n",
        "\n",
        "print(\"(n_kernels, n_samples_test, n_voxels_mask) =\", Y_test_pred_split.shape)\n",
        "print(\"(n_kernels, n_voxels_mask) =\", split_scores_mask.shape)\n",
        "\n",
        "# extend to unfitted voxels\n",
        "n_kernels = split_scores_mask.shape[0]\n",
        "n_voxels = Y_train.shape[1]\n",
        "split_scores = np.zeros((n_kernels, n_voxels))\n",
        "split_scores[:, mask] = backend.to_numpy(split_scores_mask)\n",
        "print(\"(n_kernels, n_voxels) =\", split_scores.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then plot the split scores on a flatmap with a 2D colormap.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.viz import plot_2d_flatmap_from_mapper\n",
        "\n",
        "mapper_file = os.path.join(directory, \"mappers\", f\"{subject}_mappers.hdf\")\n",
        "ax = plot_2d_flatmap_from_mapper(split_scores[0], split_scores[1],\n",
        "                                 mapper_file, vmin=0, vmax=0.25, vmin2=0,\n",
        "                                 vmax2=0.5, label_1=feature_names[0],\n",
        "                                 label_2=feature_names[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The blue regions are better predicted by the motion-energy features, the\n",
        "orange regions are better predicted by the wordnet features, and the white\n",
        "regions are well predicted by both feature spaces.\n",
        "\n",
        "Compared to the last figure of the previous example, we see that most white\n",
        "regions have been replaced by either blue or orange regions. The banded ridge\n",
        "regression disentangled the two feature spaces in voxels where both feature\n",
        "spaces had good prediction accuracy (see previous example). For example,\n",
        "motion-energy features predict brain activity in early visual cortex, while\n",
        "wordnet features predict in semantic visual areas. For more discussions about\n",
        "these results, we refer the reader to the publications describing the banded ridge \n",
        "regression approach {cite}`nunez2019,dupre2022`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "```{bibliography}\n",
        ":filter: docname in docnames\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
