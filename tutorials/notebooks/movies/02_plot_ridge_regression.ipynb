{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Understand ridge regression and cross-validation\n\nIn the following examples, we will model the fMRI responses using a regularized\nlinear regression known as ridge regression. Before building any model, the\npresent example explains why we use ridge regression, and how to use\ncross-validation to select the appropriate regularization hyper-parameter.\n\nLinear regression is a method to model an output variable ``y`` (or target)\nusing a linear combination of some input variables ``X`` (or features). For\neach sample ``X_i`` in ``X``, the model predicts an output ``y_i* = X_i @ w``,\nwhere ``w`` is a vector of coefficients, and ``@`` is the dot product. The\nmodel is considered accurate if the prediction ``y_i*`` is close to the true\nvalue ``y_i``. Therefore, given a dataset ``(X, y)``, a good linear regression\nmodel is given by the coefficients ``b`` that minimizes the sum of squared\nerrors: ``||X_i @ w - y_i||^2 = sum_i (X_i @ w - y_i)^2``. This particular\nmodel is called \"ordinary least squares\" (OLS).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ordinary least squares (OLS)\n\nTo illustrate OLS, let's use a toy dataset with a single features x1. On the\nplot below, each dot is a sample (X_i, y_i), and the linear regression model\nis the line ``y = x1 @ w1``. On each sample, the error between the prediction\nand the true value is shown by a gray line. By summing the squared errors\nover all samples, we get a particular value of the squared loss function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.regression_toy import create_regression_toy\nfrom voxelwise_tutorials.regression_toy import plot_1d\n\nX, y = create_regression_toy(n_features=1)\nplot_1d(X, y, coefs=[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By varying the linear coefficient ``w1``, we can change the prediction\naccuracy of the model, and thus the squared loss.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_1d(X, y, coefs=[0.2])\nplot_1d(X, y, coefs=[0.7])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The linear coefficient leading to the minimum squared loss can be found\nanalytically by the following formula: ``w = (X.T @ X)^-1 @ X.T @ y``.\nThis is the OLS solution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\ncoefs_ols = np.linalg.solve(X.T @ X, X.T @ y)\nplot_1d(X, y, coefs=coefs_ols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Linear regression can also be used on more than one feature. On the next\ntoy dataset, we will use two features x1 and x2. The linear regression model\nis a now plane. Here again, summing the squared errors over all samples gives\nthe squared loss.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.regression_toy import create_regression_toy\nfrom voxelwise_tutorials.regression_toy import plot_2d\n\nX, y = create_regression_toy(n_features=2)\n\ncoefs_ols = np.linalg.solve(X.T @ X, X.T @ y)\n\nplot_2d(X, y, coefs=[0, 0], show_noiseless=False)\nplot_2d(X, y, coefs=[0.25, 0], show_noiseless=False)\nplot_2d(X, y, coefs=[0.25, 0.15], show_noiseless=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here again, the OLS solution can be found analytically with the same formula.\nNote that the OLS solution is not equal to the ground truth coefficients used\nto generate the toy dataset (black cross), because we added some noise to the\ntarget values ``y``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "coefs_ols = np.linalg.solve(X.T @ X, X.T @ y)\nplot_2d(X, y, coefs=coefs_ols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The situation becomes more interesting when the features ``X```are\ncorrelated. Here, we add a correlation between the first feature ``X[:, 0]``\nand the second feature ``X[:, 1]``. With this correlation, the squared loss\nfunction is no more isotropic, so the levels of equal loss are now ellipses.\nStarting from the OLS solution, a small change in ``w`` to the top left leads\nto a small change in the loss, whereas a small change to the top right leads\nto a large change in the loss. This anisotropy makes the OLS solution less\nrobust to noise in some directions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = create_regression_toy(n_features=2, correlation=0.9)\n\ncoefs_ols = np.linalg.solve(X.T @ X, X.T @ y)\nplot_2d(X, y, coefs=coefs_ols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the correlation is even higher, the loss function is even more\nanisotropic, and the OLS solution becomes even less stable. This can be\nunderstood mathematically by the fact that the OLS solution requires\ninverting the matrix ``X.T @ X``, which amounts to inverting the eigenvalues\n``lambda_k`` of the matrix. When the features are highly correlated, some\neigenvalues are close to zero, which reduces the stability of the inversion\n(a small change in the features can have a large effect on the result).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = create_regression_toy(n_features=2, correlation=0.99)\n\ncoefs_ols = np.linalg.solve(X.T @ X, X.T @ y)\nplot_2d(X, y, coefs=coefs_ols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ridge regression\n\nTo solve this instability, OLS can be extended to ridge regression. Ridge\nregression considers a different optimization problem, which optimizes the\nsum of the squared loss function ``||X_i @ w - y_i||^2`` and of a\nregularization term ``alpha * ||w||_2``. The ridge solution finds a balance\nbetween being close to the OLS solution and being close to the origin (``w =\n0``). In the regularization term, ``alpha`` is a positive hyperparameter that\ncontrols the regularization strength. With a small ``alpha``, the solution\nwill be close to the OLS solution, and with a large ``alpha``, the solution\nwill be further from the OLS solution and closer to the origin.\n\nTo illustrate this, the following plot shows the ridge solution for a\nparticular value of ``alpha``. The black circle shows the different values of\n``w`` leading to the same regularization value, while the blue ellipses show\nthe different different values of ``w`` leading to the same squared loss\nvalue.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = create_regression_toy(n_features=2, correlation=0.9)\n\nalpha = 23\ncoefs_ridge = np.linalg.solve(X.T @ X + np.eye(X.shape[1]) * alpha, X.T @ y)\nplot_2d(X, y, coefs_ridge, alpha=alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding a regularization term makes the solution more robust to noise. Indeed,\nthe ridge solution can be found analytically with the following formula: ``w\n= (X.T @ X + alpha * I)^-1 @ X.T @ y``. In this formula, we can see that the\ninverted matrix is now ``X.T @ X + alpha * I``, which adds a positive value\n``alpha`` to all eigenvalues ``lambda_i `` of ``X^TX`` before the matrix\ninversion. Inverting ``(lambda_i + alpha)`` instead of ``lambda_i`` reduces\nthe instability caused by small eigenvalues. This makes the ridge solution\nmore robust to noise than the OLS solution.\n\nHere, we can see that with even more correlation features, the ridge solution\nis still reasonably close to the noiseless ground truth, while the OLS\nsolution would be far off.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = create_regression_toy(n_features=2, correlation=0.999)\n\nalpha = 23\ncoefs_ridge = np.linalg.solve(X.T @ X + np.eye(X.shape[1]) * alpha, X.T @ y)\nplot_2d(X, y, coefs_ridge, alpha=alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter selection\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kernel ridge regression\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}