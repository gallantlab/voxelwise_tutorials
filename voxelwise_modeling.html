
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>References &#8212; Voxelwise modeling tutorials 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <link rel="stylesheet" type="text/css" href="_static/gallery.css" />
    <link rel="stylesheet" type="text/css" href="_static/gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="_static/gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="_static/gallery-rendered-html.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Helper Python package" href="voxelwise_package.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  <div class="document">
    
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="index.html">
    <img class="logo" src="_static/flatmap.png" alt="Logo"/>
    
  </a>
</p>



<p class="blurb">Voxelwise modeling tutorials</p>




<p>
<iframe src="https://ghbtns.com/github-btn.html?user=gallantlab&repo=voxelwise_tutorials&type=star&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="voxelwise_package.html" title="previous chapter">Helper Python package</a></li>
  </ul></li>
</ul>
</div><h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="_auto_examples/index.html">Shortclips tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="_auto_examples/index.html#vim-2-tutorial">Vim-2 tutorial</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="voxelwise_package.html">Helper Python package</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">References</a></li>
</ul>


<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="references">
<h1>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h1>
<div class="section" id="voxelwise-modeling-framework">
<h2>Voxelwise modeling framework<a class="headerlink" href="#voxelwise-modeling-framework" title="Permalink to this headline">¶</a></h2>
<p>Voxelwise modeling (VM) is a framework to perform functional magnetic resonance
imaging (fMRI) data analysis. Over the years, VM has led to many high profile
publications <a class="reference internal" href="#kay2008"><span class="std std-ref">[1]</span></a> <a class="reference internal" href="#nas2009"><span class="std std-ref">[2]</span></a> <a class="reference internal" href="#nis2011"><span class="std std-ref">[3]</span></a>
<a class="reference internal" href="#hut2012"><span class="std std-ref">[4]</span></a> <a class="reference internal" href="#cuk2013"><span class="std std-ref">[5]</span></a> <a class="reference internal" href="#cuk2013b"><span class="std std-ref">[6]</span></a>
<a class="reference internal" href="#sta2013"><span class="std std-ref">[7]</span></a> <a class="reference internal" href="#hut2016"><span class="std std-ref">[8]</span></a> <a class="reference internal" href="#deh2017"><span class="std std-ref">[9]</span></a>
<a class="reference internal" href="#les2019"><span class="std std-ref">[10]</span></a> <a class="reference internal" href="#den2019"><span class="std std-ref">[11]</span></a> <a class="reference internal" href="#nun2019"><span class="std std-ref">[12]</span></a>.</p>
</div>
<div class="section" id="critical-improvements">
<h2>Critical improvements<a class="headerlink" href="#critical-improvements" title="Permalink to this headline">¶</a></h2>
<p>VM provides multiple critical improvements over other approaches to fMRI data
analysis:</p>
<ol class="arabic simple">
<li><p>Most methods for analyzing fMRI data rely on simple contrasts between a
small number of conditions. In contrast, VM can efficiently analyze many
different stimulus and task features simultaneously. This framework enables
the analysis of complex naturalistic stimuli and tasks which contain a
large number of features; for example, VM has been used with naturalistic
images <a class="reference internal" href="#kay2008"><span class="std std-ref">[1]</span></a> <a class="reference internal" href="#nas2009"><span class="std std-ref">[2]</span></a>, shortclips
<a class="reference internal" href="#nis2011"><span class="std std-ref">[3]</span></a>, and stories <a class="reference internal" href="#hut2016"><span class="std std-ref">[8]</span></a>.</p></li>
<li><p>Unlike the traditional null hypothesis testing framework, VM is not prone
to overfitting and type I error and generalizes to new subjects and stimuli
. VM is a predictive modeling framework that evaluates model performance on
a separate test data set not used during fitting.</p></li>
<li><p>VM performs an analysis in each subject’s native brain space instead of
lossily transforming subjects into a common group space. This allows VM to
produce results with maximal spatial resolution. Each subject provides
their own fit and test data, so every subject provides a complete
replication of all hypothesis tests.</p></li>
<li><p>VM produces high-dimensional functional maps rather than simple contrast
maps or correlation matrices. These maps reflect the selectivity of each
voxel to thousands of stimulus and task features spread across dozens of
feature spaces. These functional maps are much more detailed than those
produced using statistical parametric mapping (SPM), multivariate pattern
analysis (MVPA), or representational similarity analysis (RSA).</p></li>
<li><p>VM recovers stable and interpretable functional parcellations, which
respect individual variability in anatomy <a class="reference internal" href="#hut2016"><span class="std std-ref">[8]</span></a>.</p></li>
</ol>
</div>
<div class="section" id="id1">
<h2>References<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<dl class="simple" id="kay2008">
<dt>[1] Kay, K. N., Naselaris, T., Prenger, R. J., &amp; Gallant, J. L. (2008).</dt><dd><p>Identifying natural images from human brain activity.
Nature, 452(7185), 352-355.</p>
</dd>
</dl>
<dl class="simple" id="nas2009">
<dt>[2] Naselaris, T., Prenger, R. J., Kay, K. N., Oliver, M., &amp; Gallant, J. L. (2009).</dt><dd><p>Bayesian reconstruction of natural images from human brain activity.
Neuron, 63(6), 902-915.</p>
</dd>
</dl>
<dl class="simple" id="nis2011">
<dt>[3] Nishimoto, S., Vu, A. T., Naselaris, T., Benjamini, Y., Yu, B., &amp; Gallant, J. L. (2011).</dt><dd><p>Reconstructing visual experiences from brain activity evoked by natural movies.
Current Biology, 21(19), 1641-1646.</p>
</dd>
</dl>
<dl class="simple" id="hut2012">
<dt>[4] Huth, A. G., Nishimoto, S., Vu, A. T., &amp; Gallant, J. L. (2012).</dt><dd><p>A continuous semantic space describes the representation of thousands of
object and action categories across the human brain.
Neuron, 76(6), 1210-1224.</p>
</dd>
</dl>
<dl class="simple" id="cuk2013">
<dt>[5] Çukur, T., Nishimoto, S., Huth, A. G., &amp; Gallant, J. L. (2013).</dt><dd><p>Attention during natural vision warps semantic representation across the human brain.
Nature neuroscience, 16(6), 763-770.</p>
</dd>
</dl>
<dl class="simple" id="cuk2013b">
<dt>[6] Çukur, T., Huth, A. G., Nishimoto, S., &amp; Gallant, J. L. (2013).</dt><dd><p>Functional subdomains within human FFA.
Journal of Neuroscience, 33(42), 16748-16766.</p>
</dd>
</dl>
<dl class="simple" id="sta2013">
<dt>[7] Stansbury, D. E., Naselaris, T., &amp; Gallant, J. L. (2013).</dt><dd><p>Natural scene statistics account for the representation of scene categories
in human visual cortex.
Neuron, 79(5), 1025-1034</p>
</dd>
</dl>
<dl class="simple" id="hut2016">
<dt>[8] Huth, A. G., De Heer, W. A., Griffiths, T. L., Theunissen, F. E., &amp; Gallant, J. L. (2016).</dt><dd><p>Natural speech reveals the semantic maps that tile human cerebral cortex.
Nature, 532(7600), 453-458.</p>
</dd>
</dl>
<dl class="simple" id="deh2017">
<dt>[9] de Heer, W. A., Huth, A. G., Griffiths, T. L., Gallant, J. L., &amp; Theunissen, F. E. (2017).</dt><dd><p>The hierarchical cortical organization of human speech processing.
Journal of Neuroscience, 37(27), 6539-6557.</p>
</dd>
</dl>
<dl class="simple" id="les2019">
<dt>[10] Lescroart, M. D., &amp; Gallant, J. L. (2019).</dt><dd><p>Human scene-selective areas represent 3D configurations of surfaces.
Neuron, 101(1), 178-192.</p>
</dd>
</dl>
<dl class="simple" id="den2019">
<dt>[11] Deniz, F., Nunez-Elizalde, A. O., Huth, A. G., &amp; Gallant, J. L. (2019).</dt><dd><p>The representation of semantic information across human cerebral cortex
during listening versus reading is invariant to stimulus modality.
Journal of Neuroscience, 39(39), 7722-7736.</p>
</dd>
</dl>
<dl class="simple" id="nun2019">
<dt>[12] Nunez-Elizalde, A. O., Huth, A. G., &amp; Gallant, J. L. (2019).</dt><dd><p>Voxelwise encoding models with non-spherical multivariate normal priors.
Neuroimage, 197, 482-492.</p>
</dd>
</dl>
</div>
<div class="section" id="datasets">
<h2>Datasets<a class="headerlink" href="#datasets" title="Permalink to this headline">¶</a></h2>
<dl class="simple" id="nis2011data">
<dt>[3b] Nishimoto, S., Vu, A. T., Naselaris, T., Benjamini, Y., Yu, B., &amp; Gallant, J. L. (2014).</dt><dd><p>Gallant Lab Natural Movie 4T fMRI Data.
CRCNS.org. <a class="reference external" href="http://dx.doi.org/10.6080/K00Z715X">http://dx.doi.org/10.6080/K00Z715X</a></p>
</dd>
</dl>
<dl class="simple" id="hut2012data">
<dt>[4b] Huth, A. G., Nishimoto, S., Vu, A. T., Dupré la Tour, T., &amp; Gallant, J. L. (2022).</dt><dd><p>Gallant Lab Natural Short Clips 3T fMRI Data.
GIN. <a class="reference external" href="http://dx.doi.org/10.12751/g-node.vy1zjd">http://dx.doi.org/10.12751/g-node.vy1zjd</a></p>
</dd>
</dl>
</div>
<div class="section" id="packages">
<h2>Packages<a class="headerlink" href="#packages" title="Permalink to this headline">¶</a></h2>
<dl class="simple" id="den2022">
<dt>[13] Deniz, F., Visconti di Oleggio Castello, M., Dupré La Tour, T., &amp; Gallant, J. L. (2022).</dt><dd><p>Voxelwise encoding models in functional MRI.
<em>In preparation</em>.</p>
</dd>
</dl>
<dl class="simple" id="dup2022">
<dt>[14] Dupré La Tour, T., Eickenberg, M., &amp; Gallant, J. L. (2022).</dt><dd><p>Feature-space selection with banded ridge regression.
BioRxiv. <a class="reference external" href="https://doi.org/10.1101/2022.05.05.490831">https://doi.org/10.1101/2022.05.05.490831</a></p>
</dd>
</dl>
<dl class="simple" id="gao2015">
<dt>[15] Gao, J. S., Huth, A. G., Lescroart, M. D., &amp; Gallant, J. L. (2015).</dt><dd><p>Pycortex: an interactive surface visualizer for fMRI.
Frontiers in Neuroinformatics, 23. <a class="reference external" href="https://doi.org/10.3389/fninf.2015.00023">https://doi.org/10.3389/fninf.2015.00023</a></p>
</dd>
</dl>
<dl class="simple" id="nun2021">
<dt>[16] Nunez-Elizalde, A.O., Deniz, F., Dupré la Tour, T., Visconti di Oleggio Castello, M., and Gallant, J.L. (2021).</dt><dd><p>pymoten: scientific python package for computing motion energy features from video.
Zenodo. <a class="reference external" href="https://doi.org/10.5281/zenodo.6349625">https://doi.org/10.5281/zenodo.6349625</a></p>
</dd>
</dl>
</div>
</div>


          </div>
          
        </div>
      </div>
    <div class="clearer"></div>
  </div>
    <div class="footer">
      &copy;2020, Gallant lab.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/voxelwise_modeling.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>