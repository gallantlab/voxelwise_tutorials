.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download__auto_examples_movies_3T_04_plot_motion_energy_model.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr__auto_examples_movies_3T_04_plot_motion_energy_model.py:


=============================================
Fit a ridge model with motion energy features
=============================================

In this example, we model the fMRI responses with motion-energy features
extracted from the movie stimulus. The model is a regularized linear regression
model.

This tutorial reproduces part of the analysis described in Nishimoto et al
(2011) [1]_. See this publication for more details about the experiment, the
motion-energy features, along with more results and more discussions.

*Motion-energy features:* Motion-energy features result from filtering a video
stimulus with spatio-temporal Gabor filters. A pyramid of filters is used to
compute the motion-energy features at multiple spatial and temporal scales.
Motion-energy features were introduced in [1]_.

*Summary:* As in the previous example, we first concatenate the features with
multiple delays, to account for the slow hemodynamic response. A linear
regression model then weights each delayed feature with a different weight, to
build a predictive model of BOLD activity. Again, the linear regression is
regularized to improve robustness to correlated features and to improve
generalization. The optimal regularization hyperparameter is selected
independently on each voxel over a grid-search with cross-validation. Finally,
the model generalization performance is evaluated on a held-out test set,
comparing the model predictions with the ground-truth fMRI responses.

Path of the data directory


.. code-block:: default

    import os
    from voxelwise_tutorials.io import get_data_home
    directory = os.path.join(get_data_home(), "vim-5")
    print(directory)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /home/jlg/tomdlt/voxelwise_tutorials_data/vim-5





.. code-block:: default


    # modify to use another subject
    subject = "S01"








Load the data
-------------

We first load the fMRI responses.


.. code-block:: default

    import numpy as np

    from voxelwise_tutorials.io import load_hdf5_array

    file_name = os.path.join(directory, "responses", f"{subject}_responses.hdf")
    Y_train = load_hdf5_array(file_name, key="Y_train")
    Y_test = load_hdf5_array(file_name, key="Y_test")

    print("(n_samples_train, n_voxels) =", Y_train.shape)
    print("(n_repeats, n_samples_test, n_voxels) =", Y_test.shape)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    (n_samples_train, n_voxels) = (3600, 84038)
    (n_repeats, n_samples_test, n_voxels) = (10, 270, 84038)




We average the test repeats, to remove the non-repeatable part of fMRI
responses.


.. code-block:: default

    Y_test = Y_test.mean(0)

    print("(n_samples_test, n_voxels) =", Y_test.shape)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    (n_samples_test, n_voxels) = (270, 84038)




We fill potential NaN (not-a-number) values with zeros.


.. code-block:: default

    Y_train = np.nan_to_num(Y_train)
    Y_test = np.nan_to_num(Y_test)








Then we load the precomputed "motion-energy" features.


.. code-block:: default


    feature_space = "motion_energy"
    file_name = os.path.join(directory, "features", f"{feature_space}.hdf")
    X_train = load_hdf5_array(file_name, key="X_train")
    X_test = load_hdf5_array(file_name, key="X_test")

    print("(n_samples_train, n_features) =", X_train.shape)
    print("(n_samples_test, n_features) =", X_test.shape)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    (n_samples_train, n_features) = (3600, 6555)
    (n_samples_test, n_features) = (270, 6555)




Define the cross-validation scheme
----------------------------------

We define the same leave-one-run-out cross-validation split as in the
previous example.


.. code-block:: default


    from sklearn.model_selection import check_cv
    from voxelwise_tutorials.utils import generate_leave_one_run_out

    # indice of first sample of each run
    run_onsets = load_hdf5_array(file_name, key="run_onsets")
    print(run_onsets)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [   0  300  600  900 1200 1500 1800 2100 2400 2700 3000 3300]




We define a cross-validation splitter, compatible with ``scikit-learn`` API.


.. code-block:: default

    n_samples_train = X_train.shape[0]
    cv = generate_leave_one_run_out(n_samples_train, run_onsets)
    cv = check_cv(cv)  # copy the cross-validation splitter into a reusable list








Define the model
----------------

We define the same model as in the previous example. See the previous
example for more details about the model definition.


.. code-block:: default


    from sklearn.pipeline import make_pipeline
    from sklearn.preprocessing import StandardScaler
    from voxelwise_tutorials.delayer import Delayer
    from himalaya.kernel_ridge import KernelRidgeCV
    from himalaya.backend import set_backend
    backend = set_backend("torch_cuda", on_error="warn")

    X_train = X_train.astype("float32")
    X_test = X_test.astype("float32")

    alphas = np.logspace(1, 20, 20)

    pipeline = make_pipeline(
        StandardScaler(with_mean=True, with_std=False),
        Delayer(delays=[1, 2, 3, 4]),
        KernelRidgeCV(
            alphas=alphas, cv=cv,
            solver_params=dict(n_targets_batch=500, n_alphas_batch=5,
                               n_targets_batch_refit=100)),
    )









.. code-block:: default

    from sklearn import set_config
    set_config(display='diagram')  # requires scikit-learn 0.23
    pipeline






.. only:: builder_html

    .. raw:: html

        <style>#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba {color: black;background-color: white;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba pre{padding: 0;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-toggleable {background-color: white;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-estimator:hover {background-color: #d4ebff;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-item {z-index: 1;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-parallel::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-parallel-item:only-child::after {width: 0;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-b51dc38c-1bf7-4a88-b843-a36c4708daba div.sk-container {display: inline-block;position: relative;}</style><div id="sk-b51dc38c-1bf7-4a88-b843-a36c4708daba" class"sk-top-container"><div class="sk-container"><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="5e3fd35c-ceac-4b9c-b40a-47b611f8926b" type="checkbox" ><label class="sk-toggleable__label" for="5e3fd35c-ceac-4b9c-b40a-47b611f8926b">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[('standardscaler', StandardScaler(with_std=False)),
                        ('delayer', Delayer(delays=[1, 2, 3, 4])),
                        ('kernelridgecv',
                         KernelRidgeCV(alphas=array([1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06, 1.e+07, 1.e+08,
               1.e+09, 1.e+10, 1.e+11, 1.e+12, 1.e+13, 1.e+14, 1.e+15, 1.e+16,
               1.e+17, 1.e+18, 1.e+19, 1.e+20]),
                                       cv=_CVIterableWrapper(cv=[(array([   0,    1, ..., 3598, 3599]), array...99])), (array([ 300,  301, ..., 3598, 3599]), array([  0,   1, ..., 298, 299])), (array([   0,    1, ..., 3598, 3599]), array([1500, 1501, ..., 1798, 1799])), (array([   0,    1, ..., 3598, 3599]), array([1800, 1801, ..., 209...1, ..., 3298, 3299])), (array([   0,    1, ..., 3598, 3599]), array([ 900,  901, ..., 1198, 1199]))]),
                                       solver_params={'n_alphas_batch': 5,
                                                      'n_targets_batch': 500,
                                                      'n_targets_batch_refit': 100}))])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="4459ae70-bc7f-4c3d-b160-90b6060a615a" type="checkbox" ><label class="sk-toggleable__label" for="4459ae70-bc7f-4c3d-b160-90b6060a615a">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler(with_std=False)</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="0b8325ce-caa4-4608-8046-7813febde8c3" type="checkbox" ><label class="sk-toggleable__label" for="0b8325ce-caa4-4608-8046-7813febde8c3">Delayer</label><div class="sk-toggleable__content"><pre>Delayer(delays=[1, 2, 3, 4])</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="841b30a3-c8ed-4f4d-b53f-da5740426748" type="checkbox" ><label class="sk-toggleable__label" for="841b30a3-c8ed-4f4d-b53f-da5740426748">KernelRidgeCV</label><div class="sk-toggleable__content"><pre>KernelRidgeCV(alphas=array([1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06, 1.e+07, 1.e+08,
               1.e+09, 1.e+10, 1.e+11, 1.e+12, 1.e+13, 1.e+14, 1.e+15, 1.e+16,
               1.e+17, 1.e+18, 1.e+19, 1.e+20]),
                      cv=_CVIterableWrapper(cv=[(array([   0,    1, ..., 3598, 3599]), array([1200, 1201, ..., 1498, 1499])), (array([ 300,  301, ..., 3598, 3599]), array([  0,   1, ..., 298, 299])), (array([   0,    1, ..., 3598, 3599]), array([1500, 1501, ..., 1798, 1799])), (array([   0,    1, ..., 3598, 3599]), array([1800, 1801, ..., 209...1, ..., 3298, 3299])), (array([   0,    1, ..., 3598, 3599]), array([ 900,  901, ..., 1198, 1199]))]),
                      solver_params={'n_alphas_batch': 5, 'n_targets_batch': 500,
                                     'n_targets_batch_refit': 100})</pre></div></div></div></div></div></div></div>
        <br />
        <br />

Fit the model
-------------

We fit on the train set, and score on the test set.


.. code-block:: default


    pipeline.fit(X_train, Y_train)

    scores_motion_energy = pipeline.score(X_test, Y_test)
    scores_motion_energy = backend.to_numpy(scores_motion_energy)

    print("(n_voxels,) =", scores_motion_energy.shape)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    (n_voxels,) = (84038,)




Plot the model performances
---------------------------
The performances are computed using the math:`R^2` scores.


.. code-block:: default


    import matplotlib.pyplot as plt
    from voxelwise_tutorials.viz import plot_flatmap_from_mapper

    mapper_file = os.path.join(directory, "mappers", f"{subject}_mappers.hdf")
    ax = plot_flatmap_from_mapper(scores_motion_energy, mapper_file, vmin=0,
                                  vmax=0.5)
    plt.show()




.. image:: /_auto_examples/movies_3T/images/sphx_glr_04_plot_motion_energy_model_001.png
    :alt: 04 plot motion energy model
    :class: sphx-glr-single-img





The motion-energy features lead to large generalization scores in the
early visual cortex (V1, V2, V3, ...). For more discussions about these
results, we refer the reader to the original publication [1]_.

Compare with the wordnet model
------------------------------

Interestingly, the motion-energy model performs well in different brain
regions than the semantic "wordnet" model fitted in the previous example. To
compare the two models, we first need to fit again the wordnet model.


.. code-block:: default


    feature_space = "wordnet"
    file_name = os.path.join(directory, "features", f"{feature_space}.hdf")
    X_train = load_hdf5_array(file_name, key="X_train")
    X_test = load_hdf5_array(file_name, key="X_test")

    X_train = X_train.astype("float32")
    X_test = X_test.astype("float32")








We can create an unfitted copy of the pipeline with the ``clone`` function,
or simply call fit again if we do not need to reuse the previous model.


.. code-block:: default


    if False:
        from sklearn.base import clone
        pipeline_wordnet = clone(pipeline)
        pipeline_wordnet









.. code-block:: default

    pipeline.fit(X_train, Y_train)
    scores_wordnet = pipeline.score(X_test, Y_test)
    scores_wordnet = backend.to_numpy(scores_wordnet)

    ax = plot_flatmap_from_mapper(scores_wordnet, mapper_file, vmin=0,
                                  vmax=0.5)
    plt.show()




.. image:: /_auto_examples/movies_3T/images/sphx_glr_04_plot_motion_energy_model_002.png
    :alt: 04 plot motion energy model
    :class: sphx-glr-single-img





We can also plot the comparison of model performances with a 2D histogram.
All ~70k voxels are represented in this histogram, where the diagonal
corresponds to identical performance for both models. A distibution deviating
from the diagonal means that one model has better predictive performances
than the other.


.. code-block:: default


    from voxelwise_tutorials.viz import plot_hist2d

    ax = plot_hist2d(scores_wordnet, scores_motion_energy)
    ax.set(title='Generalization R2 scores', xlabel='semantic wordnet model',
           ylabel='motion energy model')
    plt.show()




.. image:: /_auto_examples/movies_3T/images/sphx_glr_04_plot_motion_energy_model_003.png
    :alt: Generalization R2 scores
    :class: sphx-glr-single-img





Interestingly, the well predicted voxels are different in the two models.
To further describe these differences, we can plot both performances on the
same flatmap, using a 2D colormap.


.. code-block:: default


    from voxelwise_tutorials.viz import plot_2d_flatmap_from_mapper

    mapper_file = os.path.join(directory, "mappers", f"{subject}_mappers.hdf")
    ax = plot_2d_flatmap_from_mapper(scores_wordnet, scores_motion_energy,
                                     mapper_file, vmin=0, vmax=0.25, vmin2=0,
                                     vmax2=0.5, label_1="wordnet",
                                     label_2="motion energy")
    plt.show()




.. image:: /_auto_examples/movies_3T/images/sphx_glr_04_plot_motion_energy_model_004.png
    :alt: 04 plot motion energy model
    :class: sphx-glr-single-img





The blue regions are well predicted by the motion-energy features, the orange
regions are well predicted by the wordnet features, and the white regions are
well predicted by both feature spaces.

Interestingly, a large part of the visual semantic areas are not only well
predicted by the wordnet features, but also by the motion-energy features, as
indicated by the white color. Since these two features spaces encode quite
different information, two interpretations are possible. In the first
interpretation, the two feature spaces encode complementary information, and
could be used jointly to further increase the generalization performances. In
the second interpretation, both feature spaces encode the same information,
because of spurious correlation in the stimulus. For example, all faces in
the stimulus might be located in the same part of the visual field, thus a
motion-energy feature at this location might contain all the necessary
information to predict the presence of a face, without specifically encoding
for the semantic of faces.

To better disentangle the two feature spaces, we developed a joint model
called `banded ridge regression` [2]_, which fits multiple feature spaces
simultaneously with optimal regularization for each feature space. This model
is described in the next example.

References
----------

.. [1] Nishimoto, S., Vu, A. T., Naselaris, T., Benjamini, Y., Yu,
    B., & Gallant, J. L. (2011). Reconstructing visual experiences from brain
    activity evoked by natural movies. Current Biology, 21(19), 1641-1646.

.. [2] Nunez-Elizalde, A. O., Huth, A. G., & Gallant, J. L. (2019).
    Voxelwise encoding models with non-spherical multivariate normal priors.
    Neuroimage, 197, 482-492.


.. code-block:: default


    del pipeline








.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  56.457 seconds)


.. _sphx_glr_download__auto_examples_movies_3T_04_plot_motion_energy_model.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: 04_plot_motion_energy_model.py <04_plot_motion_energy_model.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: 04_plot_motion_energy_model.ipynb <04_plot_motion_energy_model.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
