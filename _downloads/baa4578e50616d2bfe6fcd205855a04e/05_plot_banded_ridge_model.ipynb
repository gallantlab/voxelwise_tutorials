{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Fit a banded ridge model with both wordnet and motion energy features\n\n\nIn this example, we model the fMRI responses with a `banded ridge regression`,\nwith two different feature spaces: motion energy, and wordnet categories.\n\n*Banded ridge regression:* Since the relative scaling of both feature spaces is\nunknown, we use two regularization hyperparameters (one per feature space) in a\nmodel called banded ridge regression [1]_. Just like with ridge regression, we\noptimize the hyperparameters over cross-validation. An efficient implementation\nof this model is available in the `himalaya\n<https://github.com/gallantlab/himalaya>`_ package.\n\n*Running time:* This example is more computationally intensive than previous\nexamples. With a GPU backend, the fitting of this model takes around 6 minutes.\nWith a CPU backend, it can last 10 times more.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Path of the data directory\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nfrom voxelwise_tutorials.io import get_data_home\ndirectory = os.path.join(get_data_home(), \"vim-5\")\nprint(directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# modify to use another subject\nsubject = \"S01\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the data\n-------------\n\nAs in the previous examples, we first load the fMRI responses, which are our\nregression targets.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom voxelwise_tutorials.io import load_hdf5_array\n\nfile_name = os.path.join(directory, \"responses\", f\"{subject}_responses.hdf\")\nY_train = load_hdf5_array(file_name, key=\"Y_train\")\nY_test = load_hdf5_array(file_name, key=\"Y_test\")\n\nprint(\"(n_samples_train, n_voxels) =\", Y_train.shape)\nprint(\"(n_repeats, n_samples_test, n_voxels) =\", Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We average the test repeats, to remove the non-repeatable part of fMRI\nresponses.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Y_test = Y_test.mean(0)\n\nprint(\"(n_samples_test, n_voxels) =\", Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We fill potential NaN (not-a-number) values with zeros.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Y_train = np.nan_to_num(Y_train)\nY_test = np.nan_to_num(Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we make sure the targets are centered.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Y_train -= Y_train.mean(0)\nY_test -= Y_test.mean(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we load both feature spaces, that are going to be used for the\nlinear regression model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feature_names = [\"wordnet\", \"motion_energy\"]\n\nXs_train = []\nXs_test = []\nn_features_list = []\nfor feature_space in feature_names:\n    file_name = os.path.join(directory, \"features\", f\"{feature_space}.hdf\")\n    Xi_train = load_hdf5_array(file_name, key=\"X_train\")\n    Xi_test = load_hdf5_array(file_name, key=\"X_test\")\n\n    Xs_train.append(Xi_train.astype(dtype=\"float32\"))\n    Xs_test.append(Xi_test.astype(dtype=\"float32\"))\n    n_features_list.append(Xi_train.shape[1])\n\n# concatenate the feature spaces\nX_train = np.concatenate(Xs_train, 1)\nX_test = np.concatenate(Xs_test, 1)\n\nprint(\"(n_samples_train, n_features_total) =\", X_train.shape)\nprint(\"(n_samples_test, n_features_total) =\", X_test.shape)\nprint(\"[n_features_wordnet, n_features_motion_energy] =\", n_features_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the cross-validation scheme\n----------------------------------\n\nWe define again a leave-one-run-out cross-validation split scheme.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import check_cv\nfrom voxelwise_tutorials.utils import generate_leave_one_run_out\n\n# indice of first sample of each run\nrun_onsets = load_hdf5_array(file_name, key=\"run_onsets\")\nprint(run_onsets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a cross-validation splitter, compatible with ``scikit-learn`` API.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples_train = X_train.shape[0]\ncv = generate_leave_one_run_out(n_samples_train, run_onsets)\ncv = check_cv(cv)  # copy the cross-validation splitter into a reusable list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the model\n----------------\n\nThe model pipeline contains similar steps than the pipeline from previous\nexamples. We remove the mean of each feature with a ``StandardScaler``,\nand add delays with a ``Delayer``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom voxelwise_tutorials.delayer import Delayer\nfrom himalaya.backend import set_backend\nbackend = set_backend(\"torch_cuda\", on_error=\"warn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To fit the banded ridge model, we use ``himalaya``'s\n``MultipleKernelRidgeCV`` model, with a separate linear kernel per feature\nspace. Similarly to ``KernelRidgeCV``, the model optimizes the\nhyperparameters over cross-validation. However, while ``KernelRidgeCV`` has\nto optimize only one hyperparameter (``alpha``), ``MultipleKernelRidgeCV``\nhas to optimize ``m`` hyperparameters, where ``m`` is the number of feature\nspaces (here ``m = 2``). To do so, the model implements two different\nsolvers, one using hyperparameter random search, and one using hyperparameter\ngradient descent. For large number of targets, we recommend using the\nrandom-search solver.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The class takes a number of common parameters during initialization, such as\n``kernels``, or ``solver``. Since the solver parameters are rather different\ndepending on the solver, they are passed in a ``solver_params`` dictionary\nparameter.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from himalaya.kernel_ridge import MultipleKernelRidgeCV\n\n# Here we will use the \"random_search\" solver.\nsolver = \"random_search\"\n\n# We can check its specific parameters in the function docstring:\nsolver_function = MultipleKernelRidgeCV.ALL_SOLVERS[solver]\nprint(\"Docstring of the function %s:\" % solver_function.__name__)\nprint(solver_function.__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The hyperparameter random-search solver separates the hyperparameters into a\nshared regularization ``alpha`` and a vector of positive kernel weights which\nsum to one. This separation of hyperparameters allows to explore efficiently\na large grid of values for ``alpha`` for each sampled kernel weights vector.\n\nWe use *20* random-search iterations to have a reasonably fast example. To\nhave better results, especially for larger number of feature spaces, one\nmight need more iterations. (Note that there is currently no stopping\ncriterion in the random-search method.)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_iter = 20\n\nalphas = np.logspace(1, 20, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Batch parameters, used to reduce the necessary GPU memory. A larger value\nwill be a bit faster, but the solver might crash if it is out of memory.\nOptimal values depend on the size of your dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_targets_batch = 200\nn_alphas_batch = 5\nn_targets_batch_refit = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We put all these parameters in a dictionary ``solver_params``, and define\nthe main estimator ``MultipleKernelRidgeCV``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "solver_params = dict(n_iter=n_iter, alphas=alphas,\n                     n_targets_batch=n_targets_batch,\n                     n_alphas_batch=n_alphas_batch,\n                     n_targets_batch_refit=n_targets_batch_refit)\n\nmkr_model = MultipleKernelRidgeCV(kernels=\"precomputed\", solver=solver,\n                                  solver_params=solver_params, cv=cv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need a bit more work than in previous examples before defining the full\npipeline, since the banded ridge model requires `multiple` precomputed\nkernels, one for each feature space. To compute them, we use the\n``ColumnKernelizer``, which can create multiple kernels from different\ncolumn of your features array. ``ColumnKernelizer`` works similarly to\n``scikit-learn``'s ``ColumnTransformer``, but instead of returning a\nconcatenation of transformed features, it returns a stack of kernels,\nas required in ``MultipleKernelRidgeCV(kernels=\"precomputed\")``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we create a different ``Kernelizer`` for each feature space.\nHere we use a linear kernel for all feature spaces, but ``ColumnKernelizer``\naccepts any ``Kernelizer``, or ``scikit-learn`` ``Pipeline`` ending with a\n``Kernelizer``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from himalaya.kernel_ridge import Kernelizer\nfrom sklearn import set_config\nset_config(display='diagram')  # requires scikit-learn 0.23\n\npreprocess_pipeline = make_pipeline(\n    StandardScaler(with_mean=True, with_std=False),\n    Delayer(delays=[1, 2, 3, 4]),\n    Kernelizer(kernel=\"linear\"),\n)\npreprocess_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The column kernelizer applies a different pipeline on each selection of\nfeatures, here defined with ``slices``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from himalaya.kernel_ridge import ColumnKernelizer\n\n# Find the start and end of each feature space in the concatenated ``X_train``.\nstart_and_end = np.concatenate([[0], np.cumsum(n_features_list)])\nslices = [\n    slice(start, end)\n    for start, end in zip(start_and_end[:-1], start_and_end[1:])\n]\nslices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kernelizers_tuples = [(name, preprocess_pipeline, slice_)\n                      for name, slice_ in zip(feature_names, slices)]\ncolumn_kernelizer = ColumnKernelizer(kernelizers_tuples)\ncolumn_kernelizer\n\n# (Note that ``ColumnKernelizer`` has a parameter ``n_jobs`` to parallelize\n# each ``Kernelizer``, yet such parallelism does not work with GPU arrays.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we can define the model pipeline.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline = make_pipeline(\n    column_kernelizer,\n    mkr_model,\n)\npipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the model\n-------------\n\nWe fit on the train set, and score on the test set.\n\nWith a GPU backend, the fitting of this model takes around 6 minutes. With a\nCPU backend, it can last 10 times more.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline.fit(X_train, Y_train)\n\nscores = pipeline.score(X_test, Y_test)\nscores = backend.to_numpy(scores)\n\nprint(\"(n_voxels,) =\", scores.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare with a ridge model\n--------------------------\n\nWe can compare with a baseline model, which does not use one hyperparameter\nper feature space, but instead shares the same hyperparameter for all feature\nspaces.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from himalaya.kernel_ridge import KernelRidgeCV\n\npipeline_baseline = make_pipeline(\n    StandardScaler(with_mean=True, with_std=False),\n    Delayer(delays=[1, 2, 3, 4]),\n    KernelRidgeCV(\n        alphas=alphas, cv=cv,\n        solver_params=dict(n_targets_batch=n_targets_batch,\n                           n_alphas_batch=n_alphas_batch,\n                           n_targets_batch_refit=n_targets_batch_refit)),\n)\npipeline_baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline_baseline.fit(X_train, Y_train)\nscores_baseline = pipeline_baseline.score(X_test, Y_test)\nscores_baseline = backend.to_numpy(scores_baseline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we plot the comparison of model performances with a 2D histogram.\nAll 70k voxels are represented in this histogram, where the diagonal\ncorresponds to identical performance for both models. A distibution deviating\nfrom the diagonal means that one model has better predictive performances\nthan the other.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom voxelwise_tutorials.viz import plot_hist2d\n\nax = plot_hist2d(scores_baseline, scores)\nax.set(title='Generalization R2 scores', xlabel='KernelRidgeCV',\n       ylabel='MultipleKernelRidgeCV')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the banded ridge model (``MultipleKernelRidgeCV``) outperforms\nthe ridge model (``KernelRidegeCV``). Indeed, banded ridge regression is able\nto find the optimal scalings of each feature space, independently on each\nvoxel. Banded ridge regression is thus able to perform a soft selection\nbetween the available feature spaces, based on the cross-validation\nperformances.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the banded ridge split\n---------------------------\n\nOn top of better performances, banded ridge regression also gives a way to\ndisentangle the two feature spaces. To do so, we take the kernel weights and\nthe ridge (dual) weights corresponding to each feature space, and use them to\nsplit the prediction on each feature space.\n\n\\begin{align}\\hat{y} = \\sum_i^m \\hat{y}_i = \\sum_i^m \\gamma_i K_i \\hat{w}\\end{align}\n\nThen, we use these split predictions to compute split math:`\\tilde{R}^2_i`\nscores, corrected so that there sum is equal to the math:`R^2` score of the\nfull prediction math:`\\hat{y}`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from himalaya.scoring import r2_score_split\n\nY_test_pred_split = pipeline.predict(X_test, split=True)\nsplit_scores = r2_score_split(Y_test, Y_test_pred_split)\nsplit_scores.shape\n\nprint(\"(n_kernels, n_samples_test, n_voxels) =\", Y_test_pred_split.shape)\nprint(\"(n_kernels, n_voxels) =\", split_scores.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then plot the split scores on a flatmap with a 2D colormap.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.viz import plot_2d_flatmap_from_mapper\n\nmapper_file = os.path.join(directory, \"mappers\", f\"{subject}_mappers.hdf\")\nax = plot_2d_flatmap_from_mapper(split_scores[0], split_scores[1],\n                                 mapper_file, vmin=0, vmax=0.25, vmin2=0,\n                                 vmax2=0.5, label_1=feature_names[0],\n                                 label_2=feature_names[1])\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The blue regions are predicted using the motion-energy features, the orange\nregions are predicted using the wordnet features, and the white regions are\nwell predicted using both feature spaces.\n\nCompared to the last figure of the previous example, we see that most white\nregions have been replaced by either blue or orange regions. Indeed, the\nbanded ridge regression disentangled the two feature spaces in voxels where\nboth feature spaces had good performances (see previous example). The\ndisentanglement is consistent with prior knowledge. For example, the\nmotion-energy features are selected in the early visual cortex, while the\nwordnet features are selected in the semantic visual areas. For more\ndiscussions about these results, we refer the reader to the original\npublication [1]_.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n----------\n\n.. [1] Nunez-Elizalde, A. O., Huth, A. G., & Gallant, J. L. (2019).\n    Voxelwise encoding models with non-spherical multivariate normal priors.\n    Neuroimage, 197, 482-492.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "del pipeline, pipeline_baseline"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}