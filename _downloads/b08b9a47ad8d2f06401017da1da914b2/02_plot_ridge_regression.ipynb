{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Understand ridge regression and cross-validation\n\nIn future examples, we will model the fMRI responses using a regularized linear\nregression known as *ridge regression*. This example explains why we use ridge\nregression, and how to use cross-validation to select the appropriate\nregularization hyper-parameter.\n\nLinear regression is a method to model the relation between some input\nvariables $X \\in \\mathbb{R}^{(n \\times p)}$ (the features) and an output\nvariable $y \\in \\mathbb{R}^{n}$ (the target). Specifically, linear\nregression uses a vector of coefficient $w \\in \\mathbb{R}^{p}$` to\npredict the output\n\n\\begin{align}\\hat{y} = Xw\\end{align}\n\nThe model is considered accurate if the predictions $\\hat{y}$ are close\nto the true output values $y$. Therefore,  a good linear regression model\nis given by the vector $w$ that minimizes the sum of squared errors:\n\n\\begin{align}w = \\arg\\min_w ||Xw - y||^2\\end{align}\n\nThis is the simplest model for linear regression, and it is known as *ordinary\nleast squares* (OLS).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ordinary least squares (OLS)\n\nTo illustrate OLS, let's use a toy dataset with a single features ``X[:,0]``.\nOn the plot below (left panel), each dot is a sample ``(X[i,0], y[i])``, and\nthe linear regression model is the line ``y = X[:,0] * w[0]``. On each\nsample, the error between the prediction and the true value is shown by a\ngray line. By summing the squared errors over all samples, we get the squared\nloss. Plotting the squared loss for every value of ``w`` leads to a parabola\n(right panel).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom voxelwise_tutorials.regression_toy import create_regression_toy\nfrom voxelwise_tutorials.regression_toy import plot_1d\n\nX, y = create_regression_toy(n_features=1)\n\nplot_1d(X, y, w=[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By varying the linear coefficient ``w``, we can change the prediction\naccuracy of the model, and thus the squared loss.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_1d(X, y, w=[0.2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_1d(X, y, w=[0.7])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The linear coefficient leading to the minimum squared loss can be found\nanalytically with the formula:\n\n\\begin{align}w = (X^\\top X)^{-1}  X^\\top y\\end{align}\n\nThis is the OLS solution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "w_ols = np.linalg.solve(X.T @ X, X.T @ y)\n\nplot_1d(X, y, w=w_ols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Linear regression can also be used on more than one feature. On the next toy\ndataset, we will use two features ``X[:,0]`` and ``X[:,1]``. The linear\nregression model is a now plane. Here again, summing the squared errors over\nall samples gives the squared loss.Plotting the squared loss for every value\nof ``w[0]`` and ``w[1]`` leads to a 2D parabola (right panel).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.regression_toy import create_regression_toy\nfrom voxelwise_tutorials.regression_toy import plot_2d\n\nX, y = create_regression_toy(n_features=2)\n\nplot_2d(X, y, w=[0, 0], show_noiseless=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_2d(X, y, w=[0.4, 0], show_noiseless=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_2d(X, y, w=[0, 0.3], show_noiseless=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here again, the OLS solution can be found analytically with the same formula.\nNote that the OLS solution is not equal to the ground-truth coefficients used\nto generate the toy dataset (black cross), because we added some noise to the\ntarget values ``y``. We want the solution we find to be as close as possible\nto the ground-truth coefficients, because it will allow the regression to\ngeneralize correctly to new data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "w_ols = np.linalg.solve(X.T @ X, X.T @ y)\nplot_2d(X, y, w=w_ols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The situation becomes more interesting when the features in ``X`` are\ncorrelated. Here, we add a correlation between the first feature ``X[:, 0]``\nand the second feature ``X[:, 1]``. With this correlation, the squared loss\nfunction is no more isotropic, so the lines of equal loss are now ellipses\ninstead of circles. Thus, when starting from the OLS solution, moving ``w``\ntoward the top left leads to a small change in the loss, whereas moving it\ntoward the top right leads to a large change in the loss. This anisotropy\nmakes the OLS solution less robust to noise in some particular directions\n(deviating more from the ground-truth coefficients).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = create_regression_toy(n_features=2, correlation=0.9)\n\nw_ols = np.linalg.solve(X.T @ X, X.T @ y)\nplot_2d(X, y, w=w_ols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The different robustness to noise can be understood mathematically by the\nfact that the OLS solution requires inverting the matrix $(X^T X)$. The\nmatrix inversion amounts to inverting the eigenvalues $\\lambda_k$ of\nthe matrix. When the features are highly correlated, some eigenvalues\n$\\lambda_k$ are close to zero, and a small change in the features can\nhave a large effect on the inverse. Thus, having small eigenvalues reduces\nthe stability of the inversion. If the correlation is even higher, the\nsmallest eigenvalues get closer to zero, and the OLS solution becomes even\nless stable.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = create_regression_toy(n_features=2, correlation=0.999)\n\nw_ols = np.linalg.solve(X.T @ X, X.T @ y)\nplot_2d(X, y, w=w_ols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The instability can become even more pronounced with larger number of\nfeatures, or with smaller numbers of samples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = create_regression_toy(n_samples=10, n_features=2, correlation=0.999)\n\nw_ols = np.linalg.solve(X.T @ X, X.T @ y)\nplot_2d(X, y, w=w_ols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When the number of features is larger than the number of samples, the linear\nsystem becomes under-determined, which means that the OLS problem has an\ninfinite number of solutions, most of which do not generalize well to new\ndata.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ridge regression\n\nTo solve the instability and under-determinacy issues of OLS, OLS can be\nextended to *ridge regression*. Ridge regression considers a different\noptimization problem:\n\n\\begin{align}w = \\arg\\min_w ||Xw - y||^2 + \\alpha ||w||^2\\end{align}\n\nThis optimization problem contains two terms: (i) a *data-fitting term*\n$||Xw - y||^2$, which ensures the regression correctly fits the\ntraining data; and (ii) a regularization term $\\alpha||w||^2$, which\nforces the coefficients $w$ to be close to zero. The regularization\nterm increases the stability of the solution, at the cost of a bias toward\nzero.\n\nIn the regularization term, ``alpha`` is a positive hyperparameter that\ncontrols the regularization strength. With a smaller ``alpha``, the solution\nwill be closer to the OLS solution, and with a larger ``alpha``, the solution\nwill be further from the OLS solution and closer to the origin.\n\nTo illustrate this effect, the following plot shows the ridge solution for a\nparticular value of ``alpha``. The black circle corresponds to the line of\nequal regularization, whereas the blue ellipses are the lines of equal\nsquared loss.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = create_regression_toy(n_features=2, correlation=0.9)\n\nalpha = 23\nw_ridge = np.linalg.solve(X.T @ X + np.eye(X.shape[1]) * alpha, X.T @ y)\nplot_2d(X, y, w_ridge, alpha=alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To understand why the regularization term makes the solution more robust to\nnoise, let's consider the ridge solution. The ridge solution can be found\nanalytically with the formula:\n\n\\begin{align}w = (X^\\top X + \\alpha I)^{-1}  X^\\top y\\end{align}\n\nwhere ``I`` is the identity matrix. In this formula, we can see that the\ninverted matrix is now $(X^\\top X + \\alpha I)$. Compared to OLS, the\nadditional term $\\alpha I$ adds a positive value ``alpha`` to all\neigenvalues $\\lambda_k$ of $(X^\\top X)$ before the matrix\ninversion. Inverting $(\\lambda_k + \\alpha)$ instead of\n$\\lambda_k$ reduces the instability caused by small eigenvalues. This\nexplains why the ridge solution is more robust to noise than the OLS\nsolution.\n\nIn the following plots, we can see that even with a stronger correlation, the\nridge solution is still reasonably close to the noiseless ground truth, while\nthe OLS solution would be far off.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = create_regression_toy(n_features=2, correlation=0.999)\n\nalpha = 23\nw_ridge = np.linalg.solve(X.T @ X + np.eye(X.shape[1]) * alpha, X.T @ y)\nplot_2d(X, y, w_ridge, alpha=alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Changing the regularization hyperparameter $\\alpha$ leads to another\nridge solution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = create_regression_toy(n_features=2, correlation=0.999)\n\nalpha = 200\nw_ridge = np.linalg.solve(X.T @ X + np.eye(X.shape[1]) * alpha, X.T @ y)\nplot_2d(X, y, w_ridge, alpha=alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Side note: For every $\\alpha$, at the corresponding ridge solution, the\nline of equal regularization and the line of equal loss are tangent. If the\ntwo lines were crossing, one could improve the ridge solution by moving along\none line. It would improve one term while keeping the other term constant.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter selection\nOne issue with ridge regression is that the hyperparameter $\\alpha$ is\narbitrary. Different choices of hyperparameter lead to different models. To\ncompare these models, we cannot compare the ability to fit the training data,\nbecause the best model would just be OLS ($alpha = 0$). Instead, we\nwant to compare the ability of each model to generalize to new data. To\nestimate a model ability to generalize, we can compute its prediction\naccuracy on a separate dataset that was not used during the model fitting\n(i.e. not used to find the coefficients $w$).\n\nTo illustrate this idea, we can split the data into two subsets.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.regression_toy import create_regression_toy\nfrom voxelwise_tutorials.regression_toy import plot_kfold2\n\nX, y = create_regression_toy(n_features=1)\n\nplot_kfold2(X, y, fit=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we can fit a model on each subset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "alpha = 0.1\nplot_kfold2(X, y, alpha, fit=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And compute the prediction accuracy of each model on the other subset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_kfold2(X, y, alpha, fit=True, flip=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this way, we can evaluate the ridge regression (fit with a specific\n$\\alpha$) on its ability to generalize to new data. If we do that for\ndifferent hyperparameter candidates $\\alpha$, we can select the model\nleading to the best out-of-set prediction accuracy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.regression_toy import plot_cv_path\n\nnoise = 0.1\nX, y = create_regression_toy(n_features=2, noise=noise)\nplot_cv_path(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the example above, the noise level is low, so the best hyperparameter\nalpha is close to zero, and ridge regression is not much better than OLS.\nHowever, if the dataset has more noise, a lower number of samples, or more\ncorrelated features, the best hyperparameter can be higher. In this case,\nridge regression is better than OLS.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "noise = 3\nX, y = create_regression_toy(n_features=2, noise=noise)\nplot_cv_path(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When the noise level is too high, the best hyperparameter can be the largest\non the grid. It either means that the grid is too small, or that the\nregression does not find a predictive link between the features and the\ntarget. In this case, the model with the lowest generalization error always\npredict zero ($w=0$).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "noise = 100\nX, y = create_regression_toy(n_features=2, noise=noise)\nplot_cv_path(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To summarize, to select the best hyperparameter $\\alpha$, the standard\nmethod is to perform a grid search:\n\n  - Split the training set into two subsets: one subset used to fit the\n    models, and one subset to estimate the prediction accuracy (*validation\n    set*)\n  - Define a number of hyperparameter candidates, for example [0.1, 1, 10,\n    100].\n  - Fit a separate ridge model with each hyperparameter candidate\n    $\\alpha$.\n  - Compute the prediction accuracy on the validation set.\n  - Select the hyperparameter candidate leading to the best validation\n    accuracy.\n\nTo make the grid search less sensitive to the choice of how the training data\nwas split, the process can be repeated for multiple splits. Then, the\ndifferent prediction accuracies can be averaged over splits before the\nhyperparameter selection. Thus, the process is called a *cross-validation*.\n\nLearn more about hyperparameter selection and cross-validation on the\n[scikit-learn documentation](https://scikit-learn.org/stable/modules/cross_validation.html).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}