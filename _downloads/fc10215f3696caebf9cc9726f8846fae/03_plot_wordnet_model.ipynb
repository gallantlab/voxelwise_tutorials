{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Fit a ridge model with wordnet features\n\nIn this example, we model the fMRI responses with semantic \"wordnet\" features,\nmanually annotated on each frame of the movie stimulus. The model is a\nregularized linear regression model, known as ridge regression. Since this\nmodel is used to predict brain activity from the stimulus, it is called a\n(voxelwise) encoding model.\n\nThis example reproduces part of the analysis described in Huth et al (2012)\n[1]_. See this publication for more details about the experiment, the wordnet\nfeatures, along with more results and more discussions.\n\n*Wordnet features:* The features used in this example are semantic labels\nmanually annotated on each frame of the movie stimulus. The semantic labels\ninclude nouns (such as \"woman\", \"car\", or \"building\") and verbs (such as\n\"talking\", \"touching\", or \"walking\"), for a total of 1705 distinct category\nlabels. To interpret our model, labels can be organized in a graph of semantic\nrelashionship based on the [Wordnet](https://wordnet.princeton.edu/) dataset.\n\n*Summary:* We first concatenate the features with multiple temporal delays to\naccount for the slow hemodynamic response. We then use linear regression to fit\na predictive model of brain activity. The linear regression is regularized to\nimprove robustness to correlated features and to improve generalization\nperformance. The optimal regularization hyperparameter is selected over a\ngrid-search with cross-validation. Finally, the model generalization\nperformance is evaluated on a held-out test set, comparing the model\npredictions to the corresponding ground-truth fMRI responses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Path of the data directory\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.io import get_data_home\ndirectory = get_data_home(dataset=\"shortclips\")\nprint(directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# modify to use another subject\nsubject = \"S01\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the data\n\nWe first load the fMRI responses. These responses have been preprocessed as\ndecribed in [1]_. The data is separated into a training set ``Y_train`` and a\ntesting set ``Y_test``. The training set is used for fitting models, and\nselecting the best models and hyperparameters. The test set is later used\nto estimate the generalization performance of the selected model. The\ntest set contains multiple repetitions of the same experiment to estimate\nan upper bound of the model prediction accuracy (cf. previous example).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport numpy as np\nfrom voxelwise_tutorials.io import load_hdf5_array\n\nfile_name = os.path.join(directory, \"responses\", f\"{subject}_responses.hdf\")\nY_train = load_hdf5_array(file_name, key=\"Y_train\")\nY_test = load_hdf5_array(file_name, key=\"Y_test\")\n\nprint(\"(n_samples_train, n_voxels) =\", Y_train.shape)\nprint(\"(n_repeats, n_samples_test, n_voxels) =\", Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we repeat an experiment multiple times, part of the fMRI responses might\nchange. However the modeling features do not change over the repeats, so the\nvoxelwise encoding model will predict the same signal for each repeat. To\nhave an upper bound of the model prediction accuracy, we keep only the\nrepeatable part of the signal by averaging the test repeats.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Y_test = Y_test.mean(0)\n\nprint(\"(n_samples_test, n_voxels) =\", Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We fill potential NaN (not-a-number) values with zeros.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Y_train = np.nan_to_num(Y_train)\nY_test = np.nan_to_num(Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we load the semantic \"wordnet\" features, extracted from the stimulus at\neach time point. The features corresponding to the training set are noted\n``X_train``, and the features corresponding to the test set are noted\n``X_test``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feature_space = \"wordnet\"\n\nfile_name = os.path.join(directory, \"features\", f\"{feature_space}.hdf\")\nX_train = load_hdf5_array(file_name, key=\"X_train\")\nX_test = load_hdf5_array(file_name, key=\"X_test\")\n\nprint(\"(n_samples_train, n_features) =\", X_train.shape)\nprint(\"(n_samples_test, n_features) =\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the cross-validation scheme\n\nTo select the best hyperparameter through cross-validation, we must define a\ncross-validation splitting scheme. Because fMRI time-series are\nautocorrelated in time, we should preserve as much as possible the temporal\ncorrelation. In other words, because consecutive time samples are correlated,\nwe should not put one time sample in the training set and the immediately\nfollowing time sample in the validation set. Thus, we define here a\nleave-one-run-out cross-validation split that keeps each recording run\nintact.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import check_cv\nfrom voxelwise_tutorials.utils import generate_leave_one_run_out\n\n# indice of first sample of each run\nrun_onsets = load_hdf5_array(file_name, key=\"run_onsets\")\nprint(run_onsets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a cross-validation splitter, compatible with ``scikit-learn`` API.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples_train = X_train.shape[0]\ncv = generate_leave_one_run_out(n_samples_train, run_onsets)\ncv = check_cv(cv)  # copy the cross-validation splitter into a reusable list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the model\n\nNow, let's define the model pipeline.\n\nWe first center the features, since we will not use an intercept. The mean\nvalue in fMRI recording is non-informative, so each run is detrended and\ndemeaned independently, and we do not need to predict an intercept value in\nthe linear model.\n\nHowever, we prefer to avoid normalizing by the standard deviation of each\nfeature. If the features are extracted in a consistent way from the stimulus,\ntheir relative scale is meaningful. Normalizing them independently from each\nother would remove this information. Moreover, the wordnet features are\none-hot-encoded, which means that each feature is either present (1) or not\npresent (0) in each sample. Normalizing one-hot-encoded features is not\nrecommended, since it would scale disproportionately the infrequent features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler(with_mean=True, with_std=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we concatenate the features with multiple delays to account for the\nhemodynamic response. Due to neurovascular coupling, the recorded BOLD signal\nis delayed in time with respect to the stimulus onset. With different delayed\nversions of the features, the linear regression model will weigh each delayed\nfeature with a different weight to maximize the predictions. With a sample\nevery 2 seconds, we typically use 4 delays [1, 2, 3, 4] to cover the\nhemodynamic response peak. In the next example, we further describe this\nhemodynamic response estimation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.delayer import Delayer\ndelayer = Delayer(delays=[1, 2, 3, 4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we use a ridge regression model. Ridge regression is a linear\nregression with L2 regularization. The L2 regularization improves robustness\nto correlated features and improves generalization performance. The L2\nregularization is controlled by a hyperparameter ``alpha`` that needs to be\ntuned for each dataset. This regularization hyperparameter is usually\nselected over a grid search with cross-validation, selecting the\nhyperparameter that maximizes the predictive performances on the validation\nset. See the previous example for more details about ridge regression and\nhyperparameter selection.\n\nFor computational reasons, when the number of features is larger than the\nnumber of samples, it is more efficient to solve ridge regression using the\n(equivalent) dual formulation [2]_. This dual formulation is equivalent to\nkernel ridge regression with a linear kernel. Here, we have 3600 training\nsamples, and 1705 * 4 = 6820 features (we multiply by 4 since we use 4 time\ndelays), therefore it is more efficient to use kernel ridge regression.\n\nWith one target, we could directly use the pipeline in ``scikit-learn``'s\n``GridSearchCV``, to select the optimal regularization hyperparameter\n(``alpha``) over cross-validation. However, ``GridSearchCV`` can only\noptimize a single score across all voxels (targets). Thus, in the\nmultiple-target case, ``GridSearchCV`` can only optimize (for example) the\nmean score over targets. Here, we want to find a different optimal\nhyperparameter per target/voxel, so we use the package [himalaya](https://github.com/gallantlab/himalaya) which implements a\n``scikit-learn`` compatible estimator ``KernelRidgeCV``, with hyperparameter\nselection independently on each target.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from himalaya.kernel_ridge import KernelRidgeCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``himalaya`` implements different computational backends,\nincluding two backends that use GPU for faster computations. The two\navailable GPU backends are \"torch_cuda\" and \"cupy\". (Each backend is only\navailable if you installed the corresponding package with CUDA enabled. Check\nthe ``pytorch``/``cupy`` documentation for install instructions.)\n\nHere we use the \"torch_cuda\" backend, but if the import fails we continue\nwith the default \"numpy\" backend. The \"numpy\" backend is expected to be\nslower since it only uses the CPU.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from himalaya.backend import set_backend\nbackend = set_backend(\"torch_cuda\", on_error=\"warn\")\nprint(backend)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To speed up model fitting on GPU, we use single precision float numbers.\n(This step probably does not change significantly the performances on non-GPU\nbackends.)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train = X_train.astype(\"float32\")\nX_test = X_test.astype(\"float32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the scale of the regularization hyperparameter ``alpha`` is unknown, we\nuse a large logarithmic range, and we will check after the fit that best\nhyperparameters are not all on one range edge.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "alphas = np.logspace(1, 20, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also indicate some batch sizes to limit the GPU memory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kernel_ridge_cv = KernelRidgeCV(\n    alphas=alphas, cv=cv,\n    solver_params=dict(n_targets_batch=500, n_alphas_batch=5,\n                       n_targets_batch_refit=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we use a ``scikit-learn`` ``Pipeline`` to link the different steps\ntogether. A ``Pipeline`` can be used as a regular estimator, calling\n``pipeline.fit``, ``pipeline.predict``, etc. Using a ``Pipeline`` can be\nuseful to clarify the different steps, avoid cross-validation mistakes, or\nautomatically cache intermediate results. See the ``scikit-learn``\n[documentation](https://scikit-learn.org/stable/modules/compose.html) for\nmore information.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\npipeline = make_pipeline(\n    scaler,\n    delayer,\n    kernel_ridge_cv,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can display the ``scikit-learn`` pipeline with an HTML diagram.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn import set_config\nset_config(display='diagram')  # requires scikit-learn 0.23\npipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit the model\n\nWe fit on the training set..\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_ = pipeline.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "..and score on the test set. Here the scores are the $R^2$ scores, with\nvalues in $]-\\infty, 1]$. A value of $1$ means the predictions\nare perfect.\n\nNote that since ``himalaya`` is implementing multiple-targets\nmodels, the ``score`` method differs from ``scikit-learn`` API and returns\none score per target/voxel.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = pipeline.score(X_test, Y_test)\nprint(\"(n_voxels,) =\", scores.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we fit the model on GPU, scores are returned on GPU using an array object\nspecfic to the backend we used (such as a ``torch.Tensor``). Thus, we need to\nmove them into ``numpy`` arrays on CPU, to be able to use them for example in\na ``matplotlib`` figure.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = backend.to_numpy(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the model prediction accuracy\n\nTo visualize the model prediction accuracy, we can plot it for each voxel on\na flattened surface of the brain. To do so, we use a mapper that is specific\nto the each subject's brain. (Check previous example to see how to use the\nmapper to Freesurfer average surface.)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom voxelwise_tutorials.viz import plot_flatmap_from_mapper\n\nmapper_file = os.path.join(directory, \"mappers\", f\"{subject}_mappers.hdf\")\nax = plot_flatmap_from_mapper(scores, mapper_file, vmin=0, vmax=0.4)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the \"wordnet\" features successfully predict part of the\nmeasured brain activity, with $R^2$ scores as high as 0.4. Note that\nthese scores are generalization scores, since they are computed on a test set\nthat was not used during model fitting. Since we fitted a model independently\nin each voxel, we can inspect the generalization performances at the best\navailable spatial resolution: individual voxels.\n\nThe best-predicted voxels are located in visual semantic areas like EBA, or\nFFA. This is expected since the wordnet features encode semantic information\nabout the visual stimulus. For more discussions about these results, we refer\nthe reader to the original publication [1]_.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the selected hyperparameters\n\nSince the scale of alphas is unknown, we plot the optimal alphas selected by\nthe solver over cross-validation. This plot is helpful to refine the alpha\ngrid if the range is too small or too large.\n\nNote that some voxels might be at the maximum regularization value in the\ngrid search. These are voxels where the model has no predictive power, thus\nthe optimal regularization parameter is large to lead to a prediction equal\nto zero. We do not need to extend the alpha range for these voxels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from himalaya.viz import plot_alphas_diagnostic\nbest_alphas = backend.to_numpy(pipeline[-1].best_alphas_)\nplot_alphas_diagnostic(best_alphas=best_alphas, alphas=alphas)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the regression coefficients\n\nHere, we go back to the main model on all voxels. Since our model is linear,\nwe can use the (primal) regression coefficients to interpret the model. The\nbasic intuition is that the model will use larger coefficients on features\nthat have more predictive power.\n\nSince we know the meaning of each feature, we can interpret the large\nregression coefficients. In the case of wordnet features, we can even build a\ngraph that represents the features that are linked by a semantic\nrelationship.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first get the (primal) ridge regression coefficients from the fitted\nmodel.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "primal_coef = pipeline[-1].get_primal_coef()\nprimal_coef = backend.to_numpy(primal_coef)\nprint(\"(n_delays * n_features, n_voxels) =\", primal_coef.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because the ridge model allows a different regularization per voxel, the\nregression coefficients may have very different scales. In turn, these\ndifferent scales can introduce a bias in the interpretation, focusing the\nattention disproportionately on voxels fitted with the lowest alpha. To\naddress this issue, we rescale the regression coefficient to have a norm\nequal to the square-root of the $R^2$ scores. We found empirically that\nthis rescaling best matches results obtained with a regularization shared\naccross voxels. This rescaling also removes the need to select only best\nperforming voxels, because voxels with low prediction accuracies are rescaled\nto have a low norm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "primal_coef /= np.linalg.norm(primal_coef, axis=0)[None]\nprimal_coef *= np.sqrt(np.maximum(0, scores))[None]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we aggregate the coefficients across the different delays.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# split the ridge coefficients per delays\ndelayer = pipeline.named_steps['delayer']\nprimal_coef_per_delay = delayer.reshape_by_delays(primal_coef, axis=0)\nprint(\"(n_delays, n_features, n_voxels) =\", primal_coef_per_delay.shape)\ndel primal_coef\n\n# average over delays\naverage_coef = np.mean(primal_coef_per_delay, axis=0)\nprint(\"(n_features, n_voxels) =\", average_coef.shape)\ndel primal_coef_per_delay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Even after averaging over delays, the coefficient matrix is still too large\nto interpret it. Therefore, we use principal component analysis (PCA) to\nreduce the dimensionality of the matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n\npca = PCA(n_components=4)\npca.fit(average_coef.T)\ncomponents = pca.components_\nprint(\"(n_components, n_features) =\", components.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can check the ratio of explained variance by each principal component.\nWe see that the first four components already explain a large part of the\ncoefficients variance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"PCA explained variance =\", pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly to [1]_, we correct the coefficients of features linked by a\nsemantic relationship. When building the wordnet features, if a frame was\nlabeled with `wolf`, the authors automatically added the semantically linked\ncategories `canine`, `carnivore`, `placental mammal`, `mamma`, `vertebrate`,\n`chordate`, `organism`, and `whole`. The authors thus argue that the same\ncorrection needs to be done on the coefficients.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.wordnet import load_wordnet\nfrom voxelwise_tutorials.wordnet import correct_coefficients\n_, wordnet_categories = load_wordnet(directory=directory)\ncomponents = correct_coefficients(components.T, wordnet_categories).T\ncomponents -= components.mean(axis=1)[:, None]\ncomponents /= components.std(axis=1)[:, None]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we plot the first principal component on the wordnet graph. In such\ngraph, edges indicate \"is a\" relationships (e.g. an `athlete` \"is a\"\n`person`). Each marker represents a single noun (circle) or verb (square).\nThe area of each marker indicates the principal component magnitude, and the\ncolor indicates the sign (red is positive, blue is negative).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.wordnet import plot_wordnet_graph\nfrom voxelwise_tutorials.wordnet import apply_cmap\n\nfirst_component = components[0]\nnode_sizes = np.abs(first_component)\nnode_colors = apply_cmap(first_component, vmin=-2, vmax=2, cmap='coolwarm',\n                         n_colors=2)\n\nplot_wordnet_graph(node_colors=node_colors, node_sizes=node_sizes)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "According to the authors of [1]_, \"this principal component distinguishes\nbetween categories with high stimulus energy (e.g. moving objects like\n`person` and `vehicle`) and those with low stimulus energy (e.g. stationary\nobjects like `sky` and `city`)\".\n\nIn this example, because we use only a single subject and we perform a\ndifferent voxel selection, our result is slightly different than in the\noriginal publication. We also use a different regularization parameter in\neach voxel, while in [1]_ all voxels had the same regularization parameter.\nHowever, we do not aim at reproducing exactly the results of the original\npublication, but we rather describe the general approach.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To project the principal component on the cortical surface, we first need to\nuse the fitted PCA to transform the primal weights of all voxels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# transform with the fitted PCA\naverage_coef_transformed = pca.transform(average_coef.T).T\nprint(\"(n_components, n_voxels) =\", average_coef_transformed.shape)\ndel average_coef\n\n# We make sure vmin = -vmax, so that the colormap is centered on 0.\nvmax = np.percentile(np.abs(average_coef_transformed), 99.9)\n\n# plot the primal weights projected on the first principal component.\nax = plot_flatmap_from_mapper(average_coef_transformed[0], mapper_file,\n                              vmin=-vmax, vmax=vmax, cmap='coolwarm')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This flatmap shows in which brain regions the model has the largest\nprojection on the first component. Again, this result is different from the\none in [1]_, and should only be considered as reproducing the general\napproach.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Following [1]_, we also plot the next three principal components on the\nwordnet graph, mapping the three vectors to RGB colors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.wordnet import scale_to_rgb_cube\n\nnext_three_components = components[1:4].T\nnode_sizes = np.linalg.norm(next_three_components, axis=1)\nnode_colors = scale_to_rgb_cube(next_three_components)\nprint(\"(n_nodes, n_channels) =\", node_colors.shape)\n\nplot_wordnet_graph(node_colors=node_colors, node_sizes=node_sizes)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "According to the authors of [1]_, \"this graph shows that categories thought\nto be semantically related (e.g. athletes and walking) are represented\nsimilarly in the brain\".\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we project these principal components on the cortical surface.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.viz import plot_3d_flatmap_from_mapper\n\nvoxel_colors = scale_to_rgb_cube(average_coef_transformed[1:4].T, clip=3).T\nprint(\"(n_channels, n_voxels) =\", voxel_colors.shape)\n\nax = plot_3d_flatmap_from_mapper(voxel_colors[0], voxel_colors[1],\n                                 voxel_colors[2], mapper_file=mapper_file,\n                                 vmin=0, vmax=1, vmin2=0, vmax2=1, vmin3=0,\n                                 vmax3=1)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, our results are different from the ones in [1]_, for the same reasons\nmentioned earlier.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n.. [1] Huth, A. G., Nishimoto, S., Vu, A. T., & Gallant, J. L. (2012).\n   A continuous semantic space describes the representation of thousands of\n   object and action categories across the human brain. Neuron, 76(6),\n   1210-1224.\n\n.. [2] Saunders, C., Gammerman, A., & Vovk, V. (1998).\n   Ridge regression learning algorithm in dual variables.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}